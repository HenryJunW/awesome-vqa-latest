@inproceedings{lu202012,
  title={12-in-1: Multi-task vision and language representation learning},
  author={Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10437--10446},
  year={2020}
}

@inproceedings{kembhavi2016diagram,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={European Conference on Computer Vision},
  pages={235--251},
  year={2016},
  organization={Springer}
}

@article{ilievski2016focused,
  title={A focused dynamic attention model for visual question answering},
  author={Ilievski, Ilija and Yan, Shuicheng and Feng, Jiashi},
  journal={arXiv preprint arXiv:1604.01485},
  year={2016}
}

@inproceedings{malinowski2014multi,
  title={A multi-world approach to question answering about real-world scenes based on uncertain input},
  author={Malinowski, Mateusz and Fritz, Mario},
  booktitle={Advances in neural information processing systems},
  pages={1682--1690},
  year={2014}
}

@article{shrestha2020negative,
  title={A negative case analysis of visual grounding methods for VQA},
  author={Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  journal={arXiv preprint arXiv:2004.05704},
  year={2020}
}

@inproceedings{huang2019novel,
  title={A novel framework for robustness analysis of visual qa models},
  author={Huang, Jia-Hong and Dao, Cuong Duc and Alfadly, Modar and Ghanem, Bernard},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={8449--8456},
  year={2019}
}

@inproceedings{santoro2017simple,
  title={A simple neural network module for relational reasoning},
  author={Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  booktitle={Advances in neural information processing systems},
  pages={4967--4976},
  year={2017}
}

@article{chen2015abc,
  title={Abc-cnn: An attention based convolutional neural network for visual question answering},
  author={Chen, Kan and Wang, Jiang and Chen, Liang-Chieh and Gao, Haoyuan and Xu, Wei and Nevatia, Ram},
  journal={arXiv preprint arXiv:1511.05960},
  year={2015}
}

@article{grand2019adversarial,
  title={Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects},
  author={Grand, Gabriel and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:1906.08430},
  year={2019}
}

@inproceedings{kafle2017analysis,
  title={An analysis of visual question answering algorithms},
  author={Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1965--1973},
  year={2017}
}

@inproceedings{ramakrishnan2017empirical,
  title={An empirical evaluation of visual question answering for novel objects},
  author={Ramakrishnan, Santhosh K and Pal, Ambar and Sharma, Gaurav and Mittal, Anurag},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4392--4401},
  year={2017}
}

@article{zhang2019empirical,
  title={An empirical study on leveraging scene graphs for visual question answering},
  author={Zhang, Cheng and Chao, Wei-Lun and Xuan, Dong},
  journal={arXiv preprint arXiv:1907.12133},
  year={2019}
}

@article{agrawal2016analyzing,
  title={Analyzing the behavior of visual question answering models},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1606.07356},
  year={2016}
}

@inproceedings{shrestha2019answer,
  title={Answer them all! toward universal visual question answering models},
  author={Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={10472--10481},
  year={2019}
}

@inproceedings{kafle2016answer,
  title={Answer-type prediction for visual question answering},
  author={Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4976--4984},
  year={2016}
}

@inproceedings{van2019disentangled,
  title={Are Disentangled Representations Helpful for Abstract Visual Reasoning?},
  author={van Steenkiste, Sjoerd and Locatello, Francesco and Schmidhuber, J{\"u}rgen and Bachem, Olivier},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14245--14258},
  year={2019}
}

@inproceedings{kembhavi2017you,
  title={Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension},
  author={Kembhavi, Aniruddha and Seo, Minjoon and Schwenk, Dustin and Choi, Jonghyun and Farhadi, Ali and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4999--5007},
  year={2017}
}

@inproceedings{wu2016ask,
  title={Ask me anything: Free-form visual question answering based on knowledge from external sources},
  author={Wu, Qi and Wang, Peng and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4622--4630},
  year={2016}
}

@inproceedings{xu2016ask,
  title={Ask, attend and answer: Exploring question-guided spatial attention for visual question answering},
  author={Xu, Huijuan and Saenko, Kate},
  booktitle={European Conference on Computer Vision},
  pages={451--466},
  year={2016},
  organization={Springer}
}

@article{zhang2017asking,
  title={Asking the difficult questions: Goal-oriented visual question generation via intermediate rewards},
  author={Zhang, Junjie and Wu, Qi and Shen, Chunhua and Zhang, Jian and Lu, Jianfeng and Hengel, Anton van den},
  journal={arXiv preprint arXiv:1711.07614},
  year={2017}
}

@article{huang2019assessing,
  title={Assessing the Robustness of Visual Question Answering},
  author={Huang, Jia-Hong and Alfadly, Modar and Ghanem, Bernard and Worring, Marcel},
  journal={arXiv preprint arXiv:1912.01452},
  year={2019}
}

@article{singh2018attention,
  title={Attention on attention: Architectures for visual question answering (vqa)},
  author={Singh, Jasdeep and Ying, Vincent and Nutkiewicz, Alex},
  journal={arXiv preprint arXiv:1803.07724},
  year={2018}
}

@article{chao2017being,
  title={Being negative but constructively: Lessons learnt from creating better visual question answering datasets},
  author={Chao, Wei-Lun and Hu, Hexiang and Sha, Fei},
  journal={arXiv preprint arXiv:1704.07121},
  year={2017}
}

@article{yu2018beyond,
  title={Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering},
  author={Yu, Zhou and Yu, Jun and Xiang, Chenchao and Fan, Jianping and Tao, Dacheng},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={12},
  pages={5947--5959},
  year={2018},
  publisher={IEEE}
}

@inproceedings{kim2018bilinear,
  title={Bilinear attention networks},
  author={Kim, Jin-Hwa and Jun, Jaehyun and Zhang, Byoung-Tak},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1564--1574},
  year={2018}
}

@article{guo2019bilinear,
  title={Bilinear Graph Networks for Visual Question Answering},
  author={Guo, Dalu and Xu, Chang and Tao, Dacheng},
  journal={arXiv},
  pages={arXiv--1907},
  year={2019}
}

@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6077--6086},
  year={2018}
}

@article{fang2019btdp,
  title={BTDP: Toward Sparse Fusion with Block Term Decomposition Pooling for Visual Question Answering},
  author={Fang, Zhiwei and Liu, Jing and Liu, Xueliang and Tang, Qu and Li, Yong and Lu, Hanqing},
  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  volume={15},
  number={2s},
  pages={1--21},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{zhu2015building,
  title={Building a large-scale multimodal knowledge base system for answering visual queries},
  author={Zhu, Yuke and Zhang, Ce and R{\'e}, Christopher and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1507.05670},
  year={2015}
}

@article{guo2019bilinear,
  title={Bilinear Graph Networks for Visual Question Answering},
  author={Guo, Dalu and Xu, Chang and Tao, Dacheng},
  journal={arXiv},
  pages={arXiv--1907},
  year={2019}
}

@article{agrawal2017c,
  title={C-vqa: A compositional split of the visual question answering (vqa) v1. 0 dataset},
  author={Agrawal, Aishwarya and Kembhavi, Aniruddha and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1704.08243},
  year={2017}
}

@article{ren2020cgmvqa,
  title={CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering},
  author={Ren, Fuji and Zhou, Yangyang},
  journal={IEEE Access},
  volume={8},
  pages={50626--50636},
  year={2020},
  publisher={IEEE}
}

@inproceedings{wu2018chain,
  title={Chain of reasoning for visual question answering},
  author={Wu, Chenfei and Liu, Jinlai and Wang, Xiaojie and Dong, Xuan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={275--285},
  year={2018}
}

@inproceedings{sharma2019chartnet,
  title={ChartNet: Visual Reasoning over Statistical Charts using MAC-Networks},
  author={Sharma, Monika and Gupta, Shikha and Chowdhury, Arindam and Vig, Lovekesh},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2019},
  organization={IEEE}
}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2901--2910},
  year={2017}
}

@article{yang2019co,
  title={Co-attention network with question type for visual question answering},
  author={Yang, Chao and Jiang, Mengqi and Jiang, Bin and Zhou, Weixin and Li, Keqin},
  journal={IEEE Access},
  volume={7},
  pages={40771--40781},
  year={2019},
  publisher={IEEE}
}

@article{dragomir2018combining,
  title={Combining Visual and Textual Attention in Neural Models for Enhanced Visual Question Answering},
  author={Dragomir, Cosmin and Ojog, Cristian and Rebedea, Traian},
  journal={International Journal of User-System Interaction},
  volume={11},
  number={1},
  pages={1--27},
  year={2018},
  publisher={Matrix Rom}
}

@article{kolling2020component,
  title={Component Analysis for Visual Question Answering Architectures},
  author={Kolling, Camila and Wehrmann, J{\^o}natas and Barros, Rodrigo C},
  journal={arXiv preprint arXiv:2002.05104},
  year={2020}
}

@article{hudson2018compositional,
  title={Compositional attention networks for machine reasoning},
  author={Hudson, Drew A and Manning, Christopher D},
  journal={arXiv preprint arXiv:1803.03067},
  year={2018}
}

@article{jiang2015compositional,
  title={Compositional memory for visual question answering},
  author={Jiang, Aiwen and Wang, Fang and Porikli, Fatih and Li, Yi},
  journal={arXiv preprint arXiv:1511.05676},
  year={2015}
}

@inproceedings{chen2020counterfactual,
  title={Counterfactual samples synthesizing for robust visual question answering},
  author={Chen, Long and Yan, Xin and Xiao, Jun and Zhang, Hanwang and Pu, Shiliang and Zhuang, Yueting},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10800--10809},
  year={2020}
}

@inproceedings{abbasnejad2020counterfactual,
  title={Counterfactual vision and language learning},
  author={Abbasnejad, Ehsan and Teney, Damien and Parvaneh, Amin and Shi, Javen and Hengel, Anton van den},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10044--10054},
  year={2020}
}


@article{zheng2020cross,
  title={Cross-Modality Relevance for Reasoning on Language and Vision},
  author={Zheng, Chen and Guo, Quan and Kordjamshidi, Parisa},
  journal={arXiv preprint arXiv:2005.06035},
  year={2020}
}

@inproceedings{huang2018cs,
  title={CS-VQA: visual question answering with compressively sensed images},
  author={Huang, Li-Chi and Kulkarni, Kuldeep and Jha, Anik and Lohit, Suhas and Jayasuriya, Suren and Turaga, Pavan},
  booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},
  pages={1283--1287},
  year={2018},
  organization={IEEE}
}

@inproceedings{kafle2017data,
  title={Data augmentation for visual question answering},
  author={Kafle, Kushal and Yousefhussien, Mohammed and Kanan, Christopher},
  booktitle={Proceedings of the 10th International Conference on Natural Language Generation},
  pages={198--202},
  year={2017}
}

@inproceedings{bai2018deep,
  title={Deep attention neural tensor network for visual question answering},
  author={Bai, Yalong and Fu, Jianlong and Zhao, Tiejun and Mei, Tao},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={20--35},
  year={2018}
}

@inproceedings{yu2019deep,
  title={Deep modular co-attention networks for visual question answering},
  author={Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6281--6290},
  year={2019}
}

@article{yu2020deep,
  title={Deep Multimodal Neural Architecture Search},
  author={Yu, Zhou and Cui, Yuhao and Yu, Jun and Wang, Meng and Tao, Dacheng and Tian, Qi},
  journal={arXiv preprint arXiv:2004.12070},
  year={2020}
}

@article{wu2019deep,
  title={Deep Reason: A Strong Baseline for Real-World Visual Reasoning},
  author={Wu, Chenfei and Zhou, Yanzhao and Li, Gen and Duan, Nan and Tang, Duyu and Wang, Xiaojie},
  journal={arXiv preprint arXiv:1905.10226},
  year={2019}
}

@inproceedings{liu2019densely,
  title={Densely Connected Attention Flow for Visual Question Answering.},
  author={Liu, Fei and Liu, Jing and Fang, Zhiwei and Hong, Richang and Lu, Hanqing},
  booktitle={IJCAI},
  pages={869--875},
  year={2019}
}

@article{sampat2020diverse,
  title={Diverse Visuo-Lingustic Question Answering (DVLQA) Challenge},
  author={Sampat, Shailaja and Yang, Yezhou and Baral, Chitta},
  journal={arXiv preprint arXiv:2005.00330},
  year={2020}
}

@article{mathew2020docvqa,
  title={DocVQA: A Dataset for VQA on Document Images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Manmatha, R and Jawahar, CV},
  journal={arXiv preprint arXiv:2007.00398},
  year={2020}
}

@inproceedings{agrawal2018don,
  title={Don't just assume; look and answer: Overcoming priors for visual question answering},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4971--4980},
  year={2018}
}

@article{osman2019drau,
  title={DRAU: dual recurrent attention units for visual question answering},
  author={Osman, Ahmed and Samek, Wojciech},
  journal={Computer Vision and Image Understanding},
  volume={185},
  pages={24--30},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{patro2018differential,
  title={Differential attention for visual question answering},
  author={Patro, Badri and Namboodiri, Vinay P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7680--7688},
  year={2018}
}

@inproceedings{nam2017dual,
  title={Dual attention networks for multimodal reasoning and matching},
  author={Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={299--307},
  year={2017}
}

@inproceedings{saito2017dualnet,
  title={Dualnet: Domain-invariant network for visual question answering},
  author={Saito, Kuniaki and Shin, Andrew and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle={2017 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={829--834},
  year={2017},
  organization={IEEE}
}

@inproceedings{kafle2018dvqa,
  title={DVQA: Understanding data visualizations via question answering},
  author={Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5648--5656},
  year={2018}
}

@inproceedings{xiong2016dynamic,
  title={Dynamic memory networks for visual and textual question answering},
  author={Xiong, Caiming and Merity, Stephen and Socher, Richard},
  booktitle={International conference on machine learning},
  pages={2397--2406},
  year={2016}
}

@article{chaplot2019embodied,
  title={Embodied Multimodal Multitask Learning},
  author={Chaplot, Devendra Singh and Lee, Lisa and Salakhutdinov, Ruslan and Parikh, Devi and Batra, Dhruv},
  journal={arXiv preprint arXiv:1902.01385},
  year={2019}
}

@inproceedings{fang2018enhancing,
  title={Enhancing visual question answering using dropout},
  author={Fang, Zhiwei and Liu, Jing and Qiao, Yanyuan and Tang, Qu and Li, Yong and Lu, Hanqing},
  booktitle={Proceedings of the 26th ACM international conference on Multimedia},
  pages={1002--1010},
  year={2018}
}

@article{yu2020ernie,
  title={ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph},
  author={Yu, Fei and Tang, Jiji and Yin, Weichong and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2006.16934},
  year={2020}
}

@article{cao2019explainable,
  title={Explainable High-order Visual Question Reasoning: A New Benchmark and Knowledge-routed Network},
  author={Cao, Qingxing and Li, Bailin and Liang, Xiaodan and Lin, Liang},
  journal={arXiv preprint arXiv:1909.10128},
  year={2019}
}

@inproceedings{manjunatha2019explicit,
  title={Explicit bias discovery in visual question answering models},
  author={Manjunatha, Varun and Saini, Nirat and Davis, Larry S},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9562--9571},
  year={2019}
}

@article{wang2015explicit,
  title={Explicit knowledge-based reasoning for visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony},
  journal={arXiv preprint arXiv:1511.02570},
  year={2015}
}

@inproceedings{aditya2018explicit,
  title={Explicit reasoning over end-to-end neural architectures for visual question answering},
  author={Aditya, Somak and Yang, Yezhou and Baral, Chitta},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{qiao2018exploring,
  title={Exploring human-like attention supervision in visual question answering},
  author={Qiao, Tingting and Dong, Jianfeng and Xu, Duanqing},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{ren2015exploring,
  title={Exploring models and data for image question answering},
  author={Ren, Mengye and Kiros, Ryan and Zemel, Richard},
  booktitle={Advances in neural information processing systems},
  pages={2953--2961},
  year={2015}
}

@article{halbe2020exploring,
  title={Exploring Weaknesses of VQA Models through Attribution Driven Insights},
  author={Halbe, Shaunak},
  journal={arXiv preprint arXiv:2006.06637},
  year={2020}
}

@inproceedings{dong2018fast,
  title={Fast parameter adaptation for few-shot image captioning and visual question answering},
  author={Dong, Xuanyi and Zhu, Linchao and Zhang, De and Yang, Yi and Wu, Fei},
  booktitle={Proceedings of the 26th ACM international conference on Multimedia},
  pages={54--62},
  year={2018}
}

@inproceedings{lin2018feature,
  title={Feature Enhancement in Attention for Visual Question Answering.},
  author={Lin, Yuetan and Pang, Zhangyang and Wang, Donghui and Zhuang, Yueting},
  booktitle={IJCAI},
  pages={4216--4222},
  year={2018}
}

@article{ebrahimi2017figureqa,
  title={FigureQA: An Annotated Figure Dataset for Visual Reasoning},
  author={Ebrahimi Kahou, Samira and Michalski, Vincent and Atkinson, Adam and Kadar, Akos and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv},
  pages={arXiv--1710},
  year={2017}
}

@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{liang2018focal,
  title={Focal visual-text attention for visual question answering},
  author={Liang, Junwei and Jiang, Lu and Cao, Liangliang and Li, Li-Jia and Hauptmann, Alexander G},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6135--6143},
  year={2018}
}

@article{farazi2018known,
  title={From Known to the Unknown: Transferring Knowledge to Answer Questions about Novel Visual and Semantic Concepts},
  author={Farazi, Moshiur R and Khan, Salman H and Barnes, Nick},
  journal={arXiv preprint arXiv:1811.12772},
  year={2018}
}

@inproceedings{zellers2019recognition,
  title={From recognition to cognition: Visual commonsense reasoning},
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6720--6731},
  year={2019}
}

@article{gao2019two,
  title={From two graphs to n questions: A vqa dataset for compositional reasoning on vision and commonsense},
  author={Gao, Difei and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
  journal={arXiv preprint arXiv:1908.02962},
  year={2019}
}

@article{wang2018fvqa,
  title={Fvqa: Fact-based visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Dick, Anthony and van den Hengel, Anton},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={10},
  pages={2413--2427},
  year={2018},
  publisher={IEEE}
}

@article{lu2019good,
  title={Good, Better, Best: Textual Distractors Generation for Multi-Choice VQA via Policy Gradient},
  author={Lu, Jiaying and Ye, Xin and Ren, Yi and Yang, Yezhou},
  journal={arXiv preprint arXiv:1910.09134},
  year={2019}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6700--6709},
  year={2019}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@article{guo2019graph,
  title={Graph reasoning networks for visual question answering},
  author={Guo, Dalu and Xu, Chang and Tao, Dacheng},
  journal={arXiv preprint arXiv:1907.09815},
  year={2019}
}

@inproceedings{teney2017graph,
  title={Graph-structured representations for visual question answering},
  author={Teney, Damien and Liu, Lingqiao and van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2017}
}

@article{kim2016hadamard,
  title={Hadamard product for low-rank bilinear pooling},
  author={Kim, Jin-Hwa and On, Kyoung-Woon and Lim, Woosang and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
  journal={arXiv preprint arXiv:1610.04325},
  year={2016}
}

@article{malinowski2015hard,
  title={Hard to cheat: A turing test based on answering questions about images},
  author={Malinowski, Mateusz and Fritz, Mario},
  journal={arXiv preprint arXiv:1501.03302},
  year={2015}
}

@inproceedings{yu2019heterogeneous,
  title={Heterogeneous Graph Learning for Visual Commonsense Reasoning},
  author={Yu, Weijiang and Zhou, Jingwen and Yu, Weihao and Liang, Xiaodan and Xiao, Nong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2769--2779},
  year={2019}
}

@inproceedings{lu2016hierarchical,
  title={Hierarchical question-image co-attention for visual question answering},
  author={Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  booktitle={Advances in neural information processing systems},
  pages={289--297},
  year={2016}
}

@inproceedings{schwartz2017high,
  title={High-order attention models for visual question answering},
  author={Schwartz, Idan and Schwing, Alexander and Hazan, Tamir},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3664--3674},
  year={2017}
}

@inproceedings{kuhnle2018clever,
  title={How clever is the FiLM model, and how clever can it be},
  author={Kuhnle, Alexander and Xie, Huiyuan and Copestake, Ann},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={0--0},
  year={2018}
}

@article{das2017human,
  title={Human attention in visual question answering: Do humans and deep networks look at the same regions?},
  author={Das, Abhishek and Agrawal, Harsh and Zitnick, Larry and Parikh, Devi and Batra, Dhruv},
  journal={Computer Vision and Image Understanding},
  volume={163},
  pages={90--100},
  year={2017},
  publisher={Elsevier}
}

@article{wu2017image,
  title={Image captioning and visual question answering based on attributes and external knowledge},
  author={Wu, Qi and Shen, Chunhua and Wang, Peng and Dick, Anthony and van den Hengel, Anton},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={6},
  pages={1367--1381},
  year={2017},
  publisher={IEEE}
}

@inproceedings{noh2016image,
  title={Image question answering using convolutional neural network with dynamic parameter prediction},
  author={Noh, Hyeonwoo and Hongsuck Seo, Paul and Han, Bohyung},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={30--38},
  year={2016}
}

@article{meng2017image,
  title={Image-Question-Linguistic Co-Attention for Visual Question Answering},
  author={Meng, Chenyue and Wang, Yixin and Zhang, Shutong},
  year={2017}
}

@article{qi2020imagebert,
  title={Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data},
  author={Qi, Di and Su, Lin and Song, Jia and Cui, Edward and Bharti, Taroon and Sacheti, Arun},
  journal={arXiv preprint arXiv:2001.07966},
  year={2020}
}

@inproceedings{nguyen2018improved,
  title={Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering},
  author={Nguyen, Duy-Kien and Okatani, Takayuki},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6087--6096},
  year={2018}
}

@article{wu2020improving,
  title={Improving VQA and its Explanations$\backslash$$\backslash$by Comparing Competing Explanations},
  author={Wu, Jialin and Chen, Liyan and Mooney, Raymond J},
  journal={arXiv preprint arXiv:2006.15631},
  year={2020}
}

@inproceedings{jiang2020defense,
  title={In Defense of Grid Features for Visual Question Answering},
  author={Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10267--10276},
  year={2020}
}

@article{li2017incorporating,
  title={Incorporating external knowledge to answer open-domain visual questions with dynamic memory networks},
  author={Li, Guohao and Su, Hang and Zhu, Wenwu},
  journal={arXiv preprint arXiv:1712.00733},
  year={2017}
}

@inproceedings{johnson2017inferring,
  title={Inferring and executing programs for visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Hoffman, Judy and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2989--2998},
  year={2017}
}

@article{lin2020interbert,
  title={InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining},
  author={Lin, Junyang and Yang, An and Zhang, Yichang and Liu, Jie and Zhou, Jingren and Yang, Hongxia},
  journal={arXiv preprint arXiv:2003.13198},
  year={2020}
}

@inproceedings{trott2018interpretable,
  title={Interpretable Counting for Visual Question Answering},
  author={Trott, Alexander and Xiong, Caiming and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{zhang2019interpretable,
  title={Interpretable visual question answering by visual grounding from attention supervision mining},
  author={Zhang, Yundong and Niebles, Juan Carlos and Soto, Alvaro},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={349--357},
  year={2019},
  organization={IEEE}
}

@article{goel2020iq,
  title={IQ-VQA: Intelligent Visual Question Answering},
  author={Goel, Vatsal and Chandak, Mohit and Anand, Ashish and Guha, Prithwijit},
  journal={arXiv preprint arXiv:2007.04422},
  year={2020}
}

@inproceedings{gordon2018iqa,
  title={Iqa: Visual question answering in interactive environments},
  author={Gordon, Daniel and Kembhavi, Aniruddha and Rastegari, Mohammad and Redmon, Joseph and Fox, Dieter and Farhadi, Ali},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4089--4098},
  year={2018}
}

@inproceedings{liu2018ivqa,
  title={iVQA: Inverse visual question answering},
  author={Liu, Feng and Xiang, Tao and Hospedales, Timothy M and Yang, Wankou and Sun, Changyin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8611--8619},
  year={2018}
}

@inproceedings{zhu2017knowledge,
  title={Knowledge acquisition for visual question answering via iterative querying},
  author={Zhu, Yuke and Lim, Joseph J and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1154--1163},
  year={2017}
}

@inproceedings{shah2019kvqa,
  title={Kvqa: Knowledge-aware visual question answering},
  author={Shah, Sanket and Mishra, Anand and Yadati, Naganand and Talukdar, Partha Pratim},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={8876--8884},
  year={2019}
}

@article{gan2020large,
  title={Large-Scale Adversarial Training for Vision-and-Language Representation Learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  journal={arXiv preprint arXiv:2006.06195},
  year={2020}
}

@inproceedings{hu2018learning,
  title={Learning answer embeddings for visual question answering},
  author={Hu, Hexiang and Chao, Wei-Lun and Sha, Fei},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5428--5436},
  year={2018}
}

@inproceedings{hudson2019learning,
  title={Learning by abstraction: The neural state machine},
  author={Hudson, Drew and Manning, Christopher D},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5903--5916},
  year={2019}
}

@inproceedings{norcliffe2018learning,
  title={Learning conditioned graph structures for interpretable visual question answering},
  author={Norcliffe-Brown, Will and Vafeias, Stathis and Parisot, Sarah},
  booktitle={Advances in neural information processing systems},
  pages={8334--8343},
  year={2018}
}

@article{liu2019learning,
  title={Learning Rich Image Region Representation for Visual Question Answering},
  author={Liu, Bei and Huang, Zhicheng and Zeng, Zhaoyang and Chen, Zheyu and Fu, Jianlong},
  journal={arXiv preprint arXiv:1910.13077},
  year={2019}
}

@article{pahuja2019learning,
  title={Learning Sparse Mixture of Experts for Visual Question Answering},
  author={Pahuja, Vardaan and Fu, Jie and Pal, Christopher J},
  journal={arXiv preprint arXiv:1909.09192},
  year={2019}
}

@inproceedings{ma2016learning,
  title={Learning to answer questions from image using convolutional neural network},
  author={Ma, Lin and Lu, Zhengdong and Li, Hang},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@article{andreas2016learning,
  title={Learning to compose neural networks for question answering},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  journal={arXiv preprint arXiv:1601.01705},
  year={2016}
}

@inproceedings{hu2017learning,
  title={Learning to reason: End-to-end module networks for visual question answering},
  author={Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={804--813},
  year={2017}
}

@article{zhang2018learning,
  title={Learning to count objects in natural images for visual question answering},
  author={Zhang, Yan and Hare, Jonathon and Pr{\"u}gel-Bennett, Adam},
  journal={arXiv preprint arXiv:1802.05766},
  year={2018}
}

@inproceedings{su2018learning,
  title={Learning visual knowledge memory networks for visual question answering},
  author={Su, Zhou and Zhu, Chen and Dong, Yinpeng and Cai, Dongqi and Chen, Yurong and Li, Jianguo},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7736--7745},
  year={2018}
}

@article{cao2020linguistically,
  title={Linguistically Driven Graph Capsule Network for Visual Question Reasoning},
  author={Cao, Qingxing and Liang, Xiaodan and Wang, Keze and Lin, Liang},
  journal={arXiv preprint arXiv:2003.10065},
  year={2020}
}

@article{huang2020m3p,
  title={M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training},
  author={Huang, Haoyang and Su, Lin and Qi, Di and Duan, Nan and Cui, Edward and Bharti, Taroon and Zhang, Lei and Wang, Lijuan and Gao, Jianfeng and Liu, Bei and others},
  journal={arXiv preprint arXiv:2006.02635},
  year={2020}
}

@inproceedings{goyal2017making,
  title={Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6904--6913},
  year={2017}
}

@article{zitnick2016measuring,
  title={Measuring machine intelligence through visual question answering},
  author={Zitnick, C Lawrence and Agrawal, Aishwarya and Antol, Stanislaw and Mitchell, Margaret and Batra, Dhruv and Parikh, Devi},
  journal={AI Magazine},
  volume={37},
  number={1},
  pages={63--72},
  year={2016}
}

@article{zhu2020mucko,
  title={Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based VisualQuestion Answering},
  author={Zhu, Zihao and Yu, Jing and Wang, Yujing and Sun, Yajing and Hu, Yue and Wu, Qi},
  journal={arXiv preprint arXiv:2006.09073},
  year={2020}
}

@inproceedings{yu2017multi,
  title={Multi-level attention networks for visual question answering},
  author={Yu, Dongfei and Fu, Jianlong and Mei, Tao and Rui, Yong},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4709--4717},
  year={2017}
}

@inproceedings{yu2017multi,
  title={Multi-modal factorized bilinear pooling with co-attention learning for visual question answering},
  author={Yu, Zhou and Yu, Jun and Fan, Jianping and Tao, Dacheng},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1821--1830},
  year={2017}
}

@article{fukui2016multimodal,
  title={Multimodal compact bilinear pooling for visual question answering and visual grounding},
  author={Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  journal={arXiv preprint arXiv:1606.01847},
  year={2016}
}

@article{patro2018multimodal,
  title={Multimodal differential network for visual question generation},
  author={Patro, Badri N and Kumar, Sandeep and Kurmi, Vinod K and Namboodiri, Vinay P},
  journal={arXiv preprint arXiv:1808.03986},
  year={2018}
}

@article{park2016attentive,
  title={Attentive explanations: Justifying decisions and pointing to the evidence},
  author={Park, Dong Huk and Hendricks, Lisa Anne and Akata, Zeynep and Schiele, Bernt and Darrell, Trevor and Rohrbach, Marcus},
  journal={arXiv preprint arXiv:1612.04757},
  year={2016}
}

@article{gomez2020multimodal,
  title={Multimodal grid features and cell pointers for Scene Text Visual Question Answering},
  author={G{\'o}mez, Llu{\'\i}s and Biten, Ali Furkan and Tito, Rub{\`e}n and Mafla, Andr{\'e}s and Karatzas, Dimosthenis},
  journal={arXiv preprint arXiv:2006.00923},
  year={2020}
}

@inproceedings{ilievski2017multimodal,
  title={Multimodal learning and reasoning for visual question answering},
  author={Ilievski, Ilija and Feng, Jiashi},
  booktitle={Advances in neural information processing systems},
  pages={551--562},
  year={2017}
}

@inproceedings{khademi2020multimodal,
  title={Multimodal Neural Graph Memory Networks for Visual Question Answering},
  author={Khademi, Mahmoud},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7177--7188},
  year={2020}
}

@inproceedings{kim2016multimodal,
  title={Multimodal residual learning for visual qa},
  author={Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
  booktitle={Advances in neural information processing systems},
  pages={361--369},
  year={2016}
}

@inproceedings{cadene2019murel,
  title={Murel: Multimodal relational reasoning for visual question answering},
  author={Cadene, Remi and Ben-Younes, Hedi and Cord, Matthieu and Thome, Nicolas},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1989--1998},
  year={2019}
}

@inproceedings{ben2017mutan,
  title={Mutan: Multimodal tucker fusion for visual question answering},
  author={Ben-Younes, Hedi and Cadene, R{\'e}mi and Cord, Matthieu and Thome, Nicolas},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2612--2620},
  year={2017}
}

@inproceedings{andreas2016neural,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={39--48},
  year={2016}
}

@inproceedings{yi2018neural,
  title={Neural-symbolic vqa: Disentangling reasoning from vision and language understanding},
  author={Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Josh},
  booktitle={Advances in neural information processing systems},
  pages={1031--1042},
  year={2018}
}

@article{amizadeh2020neuro,
  title={Neuro-Symbolic Visual Reasoning: Disentangling" Visual" from" Reasoning"},
  author={Amizadeh, Saeed and Palangi, Hamid and Polozov, Oleksandr and Huang, Yichen and Koishida, Kazuhito},
  journal={arXiv preprint arXiv:2006.11524},
  year={2020}
}

@inproceedings{desta2018object,
  title={Object-based reasoning in VQA},
  author={Desta, Mikyas T and Chen, Larry and Kornuta, Tomasz},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={1814--1823},
  year={2018},
  organization={IEEE}
}

@article{zhu2020object,
  title={Object-difference drived graph convolutional networks for visual question answering},
  author={Zhu, Xi and Mao, Zhendong and Chen, Zhineng and Li, Yangyang and Wang, Zhaohui and Wang, Bin},
  journal={Multimedia Tools and Applications},
  pages={1--19},
  year={2020},
  publisher={Springer}
}

@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3195--3204},
  year={2019}
}

@inproceedings{wang2020general,
  title={On the general value of evidence, and bilingual scene-text visual question answering},
  author={Wang, Xinyu and Liu, Yuliang and Shen, Chunhua and Ng, Chun Chet and Luo, Canjie and Jin, Lianwen and Chan, Chee Seng and Hengel, Anton van den and Wang, Liangwei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10126--10135},
  year={2020}
}

@article{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Hu, Xiaowei and Zhang, Pengchuan and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  journal={arXiv preprint arXiv:2004.06165},
  year={2020}
}

@inproceedings{narasimhan2018out,
  title={Out of the box: Reasoning with graph convolution nets for factual visual question answering},
  author={Narasimhan, Medhini and Lazebnik, Svetlana and Schwing, Alexander},
  booktitle={Advances in neural information processing systems},
  pages={2654--2665},
  year={2018}
}

@article{dancette2020overcoming,
  title={Overcoming Statistical Shortcuts for Open-ended Visual Counting},
  author={Dancette, Corentin and Cadene, Remi and Chen, Xinlei and Cord, Matthieu},
  journal={arXiv preprint arXiv:2006.10079},
  year={2020}
}

@inproceedings{jing2020overcoming,
  title={Overcoming Language Priors in VQA via Decomposed Linguistic Representations.},
  author={Jing, Chenchen and Wu, Yuwei and Zhang, Xiaoxun and Jia, Yunde and Wu, Qi},
  booktitle={AAAI},
  pages={11181--11188},
  year={2020}
}


@article{jolly2020p,
  title={P $$\backslash$approx $ NP, at least in Visual Question Answering},
  author={Jolly, Shailza and Palacio, Sebastian and Folz, Joachim and Raue, Federico and Hees, Jorn and Dengel, Andreas},
  journal={arXiv preprint arXiv:2003.11844},
  year={2020}
}

@article{he2020pathvqa,
  title={PathVQA: 30000+ Questions for Medical Visual Question Answering},
  author={He, Xuehai and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
  journal={arXiv preprint arXiv:2003.10286},
  year={2020}
}

@article{vedantam2019probabilistic,
  title={Probabilistic neural-symbolic models for interpretable visual question answering},
  author={Vedantam, Ramakrishna and Desai, Karan and Lee, Stefan and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1902.07864},
  year={2019}
}

@article{greco2019psycholinguistics,
  title={Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering},
  author={Greco, Claudio and Plank, Barbara and Fern{\'a}ndez, Raquel and Bernardi, Raffaella},
  journal={arXiv preprint arXiv:1906.04229},
  year={2019}
}

@article{jiang2018pythia,
  title={Pythia v0. 1: the winning entry to the vqa challenge 2018},
  author={Jiang, Yu and Natarajan, Vivek and Chen, Xinlei and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1807.09956},
  year={2018}
}

@article{prabhakar2018question,
  title={Question Relevance in Visual Question Answering},
  author={Prabhakar, Prakruthi and Kulkarni, Nitish and Zhang, Linghao},
  journal={arXiv preprint arXiv:1807.08435},
  year={2018}
}

@article{ray2016question,
  title={Question relevance in VQA: identifying non-visual and false-premise questions},
  author={Ray, Arijit and Christie, Gordon and Bansal, Mohit and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1606.06622},
  year={2016}
}

@inproceedings{gao2018question,
  title={Question-guided hybrid convolution for visual question answering},
  author={Gao, Peng and Li, Hongsheng and Li, Shuang and Lu, Pan and Li, Yikang and Hoi, Steven CH and Wang, Xiaogang},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={469--485},
  year={2018}
}

@inproceedings{lu2018r,
  title={R-VQA: learning visual relation facts with semantic attention for visual question answering},
  author={Lu, Pan and Ji, Lei and Zhang, Wei and Duan, Nan and Zhou, Ming and Wang, Jianyong},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1880--1889},
  year={2018}
}

@article{yagcioglu2018recipeqa,
  title={Recipeqa: A challenge dataset for multimodal comprehension of cooking recipes},
  author={Yagcioglu, Semih and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli},
  journal={arXiv preprint arXiv:1809.00812},
  year={2018}
}

@article{farazi2018reciprocal,
  title={Reciprocal attention fusion for visual question answering},
  author={Farazi, Moshiur R and Khan, Salman H},
  journal={arXiv preprint arXiv:1805.04247},
  year={2018}
}

@article{kv2020reducing,
  title={Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder},
  author={KV, Gouthaman and Mittal, Anurag},
  journal={arXiv preprint arXiv:2007.06198},
  year={2020}
}

@inproceedings{li2019relation,
  title={Relation-aware graph attention network for visual question answering},
  author={Li, Linjie and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={10313--10322},
  year={2019}
}

@inproceedings{jabri2016revisiting,
  title={Revisiting visual question answering baselines},
  author={Jabri, Allan and Joulin, Armand and Van Der Maaten, Laurens},
  booktitle={European conference on computer vision},
  pages={727--739},
  year={2016},
  organization={Springer}
}

@article{huang2017robustness,
  title={Robustness analysis of visual qa models by basic questions},
  author={Huang, Jia-Hong and Dao, Cuong Duc and Alfadly, Modar and Yang, C Huck and Ghanem, Bernard},
  journal={arXiv preprint arXiv:1709.04625},
  year={2017}
}

@article{lobry2020rsvqa,
  title={RSVQA: Visual Question Answering for Remote Sensing Data},
  author={Lobry, Sylvain and Marcos, Diego and Murray, Jesse and Tuia, Devis},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  year={2020},
  publisher={IEEE}
}

@inproceedings{cadene2019rubi,
  title={Rubi: Reducing unimodal biases for visual question answering},
  author={Cadene, Remi and Dancette, Corentin and Cord, Matthieu and Parikh, Devi and others},
  booktitle={Advances in neural information processing systems},
  pages={841--852},
  year={2019}
}

@article{hildebrandt2020scene,
  title={Scene Graph Reasoning for Visual Question Answering},
  author={Hildebrandt, Marcel and Li, Hang and Koner, Rajat and Tresp, Volker and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2007.01072},
  year={2020}
}

@inproceedings{wu2019self,
  title={Self-critical reasoning for robust visual question answering},
  author={Wu, Jialin and Mooney, Raymond},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8604--8614},
  year={2019}
}

@article{sur2020self,
  title={Self-Segregating and Coordinated-Segregating Transformer for Focused Deep Multi-Modular Network for Visual Question Answering},
  author={Sur, Chiranjib},
  journal={arXiv preprint arXiv:2006.14264},
  year={2020}
}

@article{kazemi2017show,
  title={Show, ask, attend, and answer: A strong baseline for visual question answering},
  author={Kazemi, Vahid and Elqursh, Ali},
  journal={arXiv preprint arXiv:1704.03162},
  year={2017}
}

@article{zhou2015simple,
  title={Simple baseline for visual question answering},
  author={Zhou, Bolei and Tian, Yuandong and Sukhbaatar, Sainbayar and Szlam, Arthur and Fergus, Rob},
  journal={arXiv preprint arXiv:1512.02167},
  year={2015}
}

@inproceedings{selvaraju2020squinting,
  title={SQuINTing at VQA Models: Introspecting VQA Models With Sub-Questions},
  author={Selvaraju, Ramprasaath R and Tendulkar, Purva and Parikh, Devi and Horvitz, Eric and Ribeiro, Marco Tulio and Nushi, Besmira and Kamar, Ece},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10003--10011},
  year={2020}
}

@inproceedings{yang2016stacked,
  title={Stacked attention networks for image question answering},
  author={Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={21--29},
  year={2016}
}

@inproceedings{narasimhan2018straight,
  title={Straight to the facts: Learning knowledge base retrieval for factual visual question answering},
  author={Narasimhan, Medhini and Schwing, Alexander G},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={451--468},
  year={2018}
}

@inproceedings{zhu2017structured,
  title={Structured attentions for visual question answering},
  author={Zhu, Chen and Zhao, Yanpeng and Huang, Shuaiyi and Tu, Kewei and Ma, Yi},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1291--1300},
  year={2017}
}

@article{gao2020structured,
  title={Structured Multimodal Attentions for TextVQA},
  author={Gao, Chenyu and Zhu, Qi and Wang, Peng and Li, Hui and Liu, Yuliang and Hengel, Anton van den and Wu, Qi},
  journal={arXiv preprint arXiv:2006.00753},
  year={2020}
}

@inproceedings{wang2018structured,
  title={Structured triplet learning with pos-tag guided attention for visual question answering},
  author={Wang, Zhe and Liu, Xiaoyi and Wang, Limin and Qiao, Yu and Xie, Xiaohui and Fowlkes, Charless},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={1888--1896},
  year={2018},
  organization={IEEE}
}

@article{gupta2017survey,
  title={Survey of visual question answering: Datasets and techniques},
  author={Gupta, Akshay Kumar},
  journal={arXiv preprint arXiv:1705.03865},
  year={2017}
}

@inproceedings{xiong2020ta,
  title={TA-Student VQA: Multi-Agents Training by Self-Questioning},
  author={Xiong, Peixi and Wu, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10065--10075},
  year={2020}
}

@inproceedings{acharya2019tallyqa,
  title={TallyQA: Answering complex counting questions},
  author={Acharya, Manoj and Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={8076--8084},
  year={2019}
}

@inproceedings{selvaraju2019taking,
  title={Taking a hint: Leveraging explanations to make vision and language models more grounded},
  author={Selvaraju, Ramprasaath R and Lee, Stefan and Shen, Yilin and Jin, Hongxia and Ghosh, Shalini and Heck, Larry and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2591--2600},
  year={2019}
}

@article{alipour2020impact,
  title={The Impact of Explanations on AI Competency Prediction in VQA},
  author={Alipour, Kamran and Ray, Arijit and Lin, Xiao and Schulze, Jurgen P and Yao, Yi and Burachas, Giedrius T},
  journal={arXiv preprint arXiv:2007.00900},
  year={2020}
}

@article{malinowski2018visual,
  title={The visual QA devil in the details: The impact of early fusion and batch norm on clevr},
  author={Malinowski, Mateusz and Doersch, Carl},
  journal={arXiv preprint arXiv:1809.04482},
  year={2018}
}

@inproceedings{wang2017vqa,
  title={The vqa-machine: Learning how to use existing vision algorithms to answer new questions},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and van den Hengel, Anton},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1173--1182},
  year={2017}
}

@article{goyal2018think,
  title={Think Visually: Question Answering through Virtual Imagery},
  author={Goyal, Ankit and Wang, Jian and Deng, Jia},
  journal={arXiv preprint arXiv:1805.11025},
  year={2018}
}

@inproceedings{teney2018tips,
  title={Tips and tricks for visual question answering: Learnings from the 2017 challenge},
  author={Teney, Damien and Anderson, Peter and He, Xiaodong and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4223--4232},
  year={2018}
}

@article{malinowski2014towards,
  title={Towards a visual turing challenge},
  author={Malinowski, Mateusz and Fritz, Mario},
  journal={arXiv preprint arXiv:1410.8027},
  year={2014}
}

@inproceedings{agarwal2020towards,
  title={Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing},
  author={Agarwal, Vedika and Shetty, Rakshith and Fritz, Mario},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9690--9698},
  year={2020}
}

@article{goyal2016towards,
  title={Towards transparent ai systems: Interpreting visual question answering models},
  author={Goyal, Yash and Mohapatra, Akrit and Parikh, Devi and Batra, Dhruv},
  journal={arXiv preprint arXiv:1608.08974},
  year={2016}
}

@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8317--8326},
  year={2019}
}

@article{noh2016training,
  title={Training recurrent answering units with joint loss minimization for vqa},
  author={Noh, Hyeonwoo and Han, Bohyung},
  journal={arXiv preprint arXiv:1606.03647},
  year={2016}
}

@inproceedings{mascharka2018transparency,
  title={Transparency by design: Closing the gap between performance and interpretability in visual reasoning},
  author={Mascharka, David and Tran, Philip and Soklaski, Ryan and Majumdar, Arjun},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4942--4950},
  year={2018}
}

@article{mogadala2019trends,
  title={Trends in integration of vision and language research: A survey of tasks, datasets, and methods},
  author={Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
  journal={arXiv preprint arXiv:1907.09358},
  year={2019}
}

@article{vatashsky2018understand,
  title={Understand, Compose and Respond-Answering Visual Questions by a Composition of Abstract Procedures},
  author={Vatashsky, Ben Zion and Ullman, Shimon},
  journal={arXiv preprint arXiv:1810.10656},
  year={2018}
}

@inproceedings{zhou2020unified,
  title={Unified Vision-Language Pre-Training for Image Captioning and VQA.},
  author={Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason J and Gao, Jianfeng},
  booktitle={AAAI},
  pages={13041--13049},
  year={2020}
}

@article{chen2019uniter,
  title={Uniter: Learning universal image-text representations},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  journal={arXiv preprint arXiv:1909.11740},
  year={2019}
}

@inproceedings{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13--23},
  year={2019}
}

@inproceedings{wang2020visual,
  title={Visual commonsense r-cnn},
  author={Wang, Tan and Huang, Jianqiang and Zhang, Hanwang and Sun, Qianru},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10760--10770},
  year={2020}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  number={1},
  pages={32--73},
  year={2017},
  publisher={Springer}
}

@article{yu2015visual,
  title={Visual madlibs: Fill in the blank image generation and question answering},
  author={Yu, Licheng and Park, Eunbyung and Berg, Alexander C and Berg, Tamara L},
  journal={arXiv preprint arXiv:1506.00278},
  year={2015}
}

@article{pollard2020visual,
  title={Visual Question Answering as a Multi-Task Problem},
  author={Pollard, Amelia Elizabeth and Shapiro, Jonathan L},
  journal={arXiv preprint arXiv:2007.01780},
  year={2020}
}

@inproceedings{li2019visual,
  title={Visual question answering as reading comprehension},
  author={Li, Hui and Wang, Peng and Shen, Chunhua and Hengel, Anton van den},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6319--6328},
  year={2019}
}

@article{bongini2020visual,
  title={Visual Question Answering for Cultural Heritage},
  author={Bongini, Pietro and Becattini, Federico and Bagdanov, Andrew D and Del Bimbo, Alberto},
  journal={arXiv preprint arXiv:2003.09853},
  year={2020}
}

@inproceedings{chou2020visual,
  title={Visual Question Answering on 360deg Images},
  author={Chou, Shih-Han and Chao, Wei-Lun and Lai, Wei-Sheng and Sun, Min and Yang, Ming-Hsuan},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  pages={1607--1616},
  year={2020}
}

@inproceedings{lioutas2018visual,
  title={Visual Question Answering using Explicit Visual Attention},
  author={Lioutas, Vasileios and Passalis, Nikolaos and Tefas, Anastasios},
  booktitle={2018 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={1--5},
  year={2018},
  organization={IEEE}
}

@inproceedings{ma2018visual,
  title={Visual question answering with memory-augmented networks},
  author={Ma, Chao and Shen, Chunhua and Dick, Anthony and Wu, Qi and Wang, Peng and van den Hengel, Anton and Reid, Ian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6975--6984},
  year={2018}
}

@article{shevchenko2020visual,
  title={Visual Question Answering with Prior Class Semantics},
  author={Shevchenko, Violetta and Teney, Damien and Dick, Anthony and Hengel, Anton van den},
  journal={arXiv preprint arXiv:2005.01239},
  year={2020}
}

@inproceedings{li2016visual,
  title={Visual question answering with question representation update (qru)},
  author={Li, Ruiyu and Jia, Jiaya},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4655--4663},
  year={2016}
}

@article{wu2017visual,
  title={Visual question answering: A survey of methods and datasets},
  author={Wu, Qi and Teney, Damien and Wang, Peng and Shen, Chunhua and Dick, Anthony and van den Hengel, Anton},
  journal={Computer Vision and Image Understanding},
  volume={163},
  pages={21--40},
  year={2017},
  publisher={Elsevier}
}

@article{kafle2017visual,
  title={Visual question answering: Datasets, algorithms, and future challenges},
  author={Kafle, Kushal and Kanan, Christopher},
  journal={Computer Vision and Image Understanding},
  volume={163},
  pages={3--20},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{cao2018visual,
  title={Visual question reasoning on general dependency tree},
  author={Cao, Qingxing and Liang, Xiaodan and Li, Bailing and Li, Guanbin and Lin, Liang},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7249--7257},
  year={2018}
}

@article{kim2018visual,
  title={Visual reasoning by progressive module networks},
  author={Kim, Seung Wook and Tapaswi, Makarand and Fidler, Sanja},
  journal={arXiv preprint arXiv:1806.02453},
  year={2018}
}

@article{geman2015visual,
  title={Visual turing test for computer vision systems},
  author={Geman, Donald and Geman, Stuart and Hallonquist, Neil and Younes, Laurent},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={12},
  pages={3618--3623},
  year={2015},
  publisher={National Acad Sciences}
}

@inproceedings{zhu2016visual7w,
  title={Visual7w: Grounded question answering in images},
  author={Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4995--5004},
  year={2016}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3608--3617},
  year={2018}
}

@inproceedings{bigham2010vizwiz,
  title={VizWiz: nearly real-time answers to visual questions},
  author={Bigham, Jeffrey P and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and others},
  booktitle={Proceedings of the 23nd annual ACM symposium on User interface software and technology},
  pages={333--342},
  year={2010}
}

@article{su2019vl,
  title={Vl-bert: Pre-training of generic visual-linguistic representations},
  author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  journal={arXiv preprint arXiv:1908.08530},
  year={2019}
}

@inproceedings{vatashsky2020vqa,
  title={VQA with no questions-answers training},
  author={Vatashsky, Ben-Zion and Ullman, Shimon},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10376--10386},
  year={2020}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@inproceedings{wu2016value,
  title={What value do explicit high level concepts have in vision to language problems?},
  author={Wu, Qi and Shen, Chunhua and Liu, Lingqiao and Dick, Anthony and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={203--212},
  year={2016}
}

@inproceedings{shih2016look,
  title={Where to look: Focus regions for visual question answering},
  author={Shih, Kevin J and Singh, Saurabh and Hoiem, Derek},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4613--4621},
  year={2016}
}

@article{terao2020visual,
  title={Which visual questions are difficult to answer? Analysis with Entropy of Answer Distributions},
  author={Terao, Kento and Tamaki, Toru and Raytchev, Bisser and Kaneda, Kazufumi and Satoh, Shun'ichi},
  journal={arXiv preprint arXiv:2004.05595},
  year={2020}
}

@inproceedings{zhang2016yin,
  title={Yin and yang: Balancing and answering binary visual questions},
  author={Zhang, Peng and Goyal, Yash and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5014--5022},
  year={2016}
}

@article{li2018zero,
  title={Zero-Shot Transfer VQA Dataset},
  author={Li, Yuanpeng and Yang, Yi and Wang, Jianyu and Xu, Wei},
  journal={arXiv preprint arXiv:1811.00692},
  year={2018}
}

@article{teney2016zero,
  title={Zero-shot visual question answering},
  author={Teney, Damien and Hengel, Anton van den},
  journal={arXiv preprint arXiv:1611.05546},
  year={2016}
}

@article{tang2020semantic,
  title={Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering},
  author={Tang, Ruixue and Ma, Chao and Zhang, Wei Emma and Wu, Qi and Yang, Xiaokang},
  journal={arXiv preprint arXiv:2007.09592},
  year={2020}
}

@article{mathew2020document,
  title={Document Visual Question Answering Challenge 2020},
  author={Mathew, Minesh and Tito, Ruben and Karatzas, Dimosthenis and Manmatha, R and Jawahar, CV},
  journal={arXiv preprint arXiv:2008.08899},
  year={2020}
}

@inproceedings{malinowski2015ask,
  title={Ask your neurons: A neural-based approach to answering questions about images},
  author={Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1--9},
  year={2015}
}

@inproceedings{chattopadhyay2017counting,
  title={Counting everyday objects in everyday scenes},
  author={Chattopadhyay, Prithvijit and Vedantam, Ramakrishna and Selvaraju, Ramprasaath R and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1135--1144},
  year={2017}
}

@inproceedings{yang2018dataset,
  title={A dataset and architecture for visual reasoning with a working memory},
  author={Yang, Guangyu Robert and Ganichev, Igor and Wang, Xiao-Jing and Shlens, Jonathon and Sussillo, David},
  booktitle={European Conference on Computer Vision},
  pages={729--745},
  year={2018},
  organization={Springer}
}

@article{mao2019neuro,
  title={The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision},
  author={Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B and Wu, Jiajun},
  journal={arXiv preprint arXiv:1904.12584},
  year={2019}
}

@inproceedings{gao2019dynamic,
  title={Dynamic fusion with intra-and inter-modality attention flow for visual question answering},
  author={Gao, Peng and Jiang, Zhengkai and You, Haoxuan and Lu, Pan and Hoi, Steven CH and Wang, Xiaogang and Li, Hongsheng},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6639--6648},
  year={2019}
}

@article{le2020dynamic,
  title={Dynamic Language Binding in Relational Visual Reasoning},
  author={Le, Thao Minh and Le, Vuong and Venkatesh, Svetha and Tran, Truyen},
  journal={arXiv preprint arXiv:2004.14603},
  year={2020}
}

@article{shi2020contrastive,
  title={Contrastive Visual-Linguistic Pretraining},
  author={Shi, Lei and Shuang, Kai and Geng, Shijie and Su, Peng and Jiang, Zhengkai and Gao, Peng and Fu, Zuohui and de Melo, Gerard and Su, Sen},
  journal={arXiv preprint arXiv:2007.13135},
  year={2020}
}

@article{kv2020linguistically,
  title={Linguistically-aware Attention for Reducing the Semantic-Gap in Vision-Language Tasks},
  author={KV, Gouthaman and Nambiar, Athira and Srinivas, Kancheti Sai and Mittal, Anurag},
  journal={arXiv preprint arXiv:2008.08012},
  year={2020}
}

@article{bansal2020visual,
  title={Visual Question Answering on Image Sets},
  author={Bansal, Ankan and Zhang, Yuting and Chellappa, Rama},
  journal={arXiv preprint arXiv:2008.11976},
  year={2020}
}

@article{yu2020cross,
  title={Cross-modal knowledge reasoning for knowledge-based visual question answering},
  author={Yu, Jing and Zhu, Zihao and Wang, Yujing and Zhang, Weifeng and Hu, Yue and Tan, Jianlong},
  journal={Pattern Recognition},
  pages={107563},
  year={2020},
  publisher={Elsevier}
}

@article{garcia_dataset_2020,
  title={A {Dataset} and {Baselines} for {Visual} {Question} {Answering} on {Art}},
  author={Garcia, Noa and Ye, Chentao and Liu, Zihua and Hu, Qingtao and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta and Mitamura, Teruko},
  journal={arXiv:2008.12520},
  year={2020},
}


@article{gao_question-led_2020,
	title = {Question-{Led} object attention for visual question answering},
	volume = {391},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219304163},
	doi = {10.1016/j.neucom.2018.11.102},
	journal = {Neurocomputing},
	author = {Gao, Lianli and Cao, Liangfu and Xu, Xing and Shao, Jie and Song, Jingkuan},
	month = may,
	year = {2020},
}

@article{hong_selective_2020,
	title = {Selective residual learning for {Visual} {Question} {Answering}},
	volume = {402},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220304859},
	doi = {10.1016/j.neucom.2020.03.098},
	journal = {Neurocomputing},
	author = {Hong, Jongkwang and Park, Sungho and Byun, Hyeran},
	year = {2020},
}

@article{toor_question_2019,
	title = {Question action relevance and editing for visual question answering},
	volume = {78},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-018-6097-z},
	journal = {Multimedia Tools and Applications},
	author = {Toor, Andeep S. and Wechsler, Harry and Nappi, Michele},
	year = {2019},
}

@article{hong_exploiting_2019,
	title = {Exploiting hierarchical visual features for visual question answering},
	volume = {351},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219303753},
	journal = {Neurocomputing},
	author = {Hong, Jongkwang and Fu, Jianlong and Uh, Youngjung and Mei, Tao and Byun, Hyeran},
	year = {2019},
}

@article{liu_inverse_2020,
	title = {Inverse {Visual} {Question} {Answering}: {A} {New} {Benchmark} and {VQA} {Diagnosis} {Tool}},
	volume = {42},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Inverse {Visual} {Question} {Answering}},
	url = {https://ieeexplore.ieee.org/document/8528867/},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Feng and Xiang, Tao and Hospedales, Timothy M. and Yang, Wankou and Sun, Changyin},
	year = {2020},
}

@article{zhu_object-difference_2020,
	title = {Object-difference drived graph convolutional networks for visual question answering},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-020-08790-0},
	journal = {Multimedia Tools and Applications},
	author = {Zhu, Xi and Mao, Zhendong and Chen, Zhineng and Li, Yangyang and Wang, Zhaohui and Wang, Bin},
	year = {2020},
}

@article{zhang_multimodal_2020,
	title = {Multimodal feature fusion by relational reasoning and attention for visual question answering},
	volume = {55},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253518308248},
	journal = {Information Fusion},
	author = {Zhang, Weifeng and Yu, Jing and Hu, Hua and Hu, Haiyang and Qin, Zengchang},
	year = {2020},
}

@article{liu_visual_2020,
	title = {Visual {Question} {Answering} via {Combining} {Inferential} {Attention} and {Semantic} {Space} {Mapping}},
	volume = {207},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120304962},
	journal = {Knowledge-Based Systems},
	author = {Liu, Yun and Zhang, Xiaoming and Huang, Feiran and Zhou, Zhibo and Zhao, Zhonghua and Li, Zhoujun},
	year = {2020},
}

@article{long_repeatpadding_2020,
	title = {{RepeatPadding}: {Balancing} words and sentence length for language comprehension in visual question answering},
	volume = {529},
	issn = {00200255},
	shorttitle = {{RepeatPadding}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002002552030342X},
	journal = {Information Sciences},
	author = {Long, Yu and Tang, Pengjie and Wei, Zhihua and Gu, Jinjing and Wang, Hanli},
	year = {2020},
}

@article{zhang_information_2019,
	title = {Information fusion in visual question answering: {A} {Survey}},
	volume = {52},
	issn = {15662535},
	shorttitle = {Information fusion in visual question answering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253518308893},
	journal = {Information Fusion},
	author = {Zhang, Dongxiang and Cao, Rui and Wu, Sai},
	year = {2019},
}

@article{ruwa_mood-aware_2019,
	title = {Mood-aware visual question answering},
	volume = {330},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231218313808},
	doi = {10.1016/j.neucom.2018.11.049},
	journal = {Neurocomputing},
	author = {Ruwa, Nelson and Mao, Qirong and Wang, Liangjun and Gou, Jianping and Dong, Ming},
	month = feb,
	year = {2019},
}

@article{peng_word--region_2019,
	title = {Word-to-region attention network for visual question answering},
	volume = {78},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-018-6389-3},
	doi = {10.1007/s11042-018-6389-3},
	journal = {Multimedia Tools and Applications},
	author = {Peng, Liang and Yang, Yang and Bin, Yi and Xie, Ning and Shen, Fumin and Ji, Yanli and Xu, Xing},
	year = {2019},
}

@article{cho_x-lxmert_2020,
	title = {X-{LXMERT}: {Paint}, {Caption} and {Answer} {Questions} with {Multi}-{Modal} {Transformers}},
	shorttitle = {X-{LXMERT}},
	url = {http://arxiv.org/abs/2009.11278},
	journal = {arXiv:2009.11278 [cs]},
	author = {Cho, Jaemin and Lu, Jiasen and Schwenk, Dustin and Hajishirzi, Hannaneh and Kembhavi, Aniruddha},
	year = {2020},
}

@article{do_multiple_2020,
	title = {Multiple interaction learning with question-type prior knowledge for constraining answer search space in visual question answering},
	url = {http://arxiv.org/abs/2009.11118},
	journal = {arXiv:2009.11118 [cs]},
	author = {Do, Tuong and Nguyen, Binh X. and Tran, Huy and Tjiputra, Erman and Tran, Quang D. and Do, Thanh-Toan},
	year = {2020},
}


@inproceedings{jang_tgif-qa_2017,
	title = {{TGIF}-{QA}: {Toward} {Spatio}-{Temporal} {Reasoning} in {Visual} {Question} {Answering}},
	shorttitle = {{TGIF}-{QA}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Jang_TGIF-QA_Toward_Spatio-Temporal_CVPR_2017_paper.html},
	urldate = {2020-01-15},
	author = {Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
	year = {2017},
	keywords = {\#},
	pages = {2758--2766},
	journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}
}

@article{gokhale_mutant_2020,
	title = {{MUTANT}: {A} {Training} {Paradigm} for {Out}-of-{Distribution} {Generalization} in {Visual} {Question} {Answering}},
	shorttitle = {{MUTANT}},
	url = {http://arxiv.org/abs/2009.08566},
	abstract = {While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present {\textbackslash}textit\{MUTANT\}, a training paradigm that exposes the model to perceptually similar, yet semantically distinct {\textbackslash}textit\{mutations\} of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, {\textbackslash}textit\{MUTANT\} does not rely on the knowledge about the nature of train and test answer distributions. {\textbackslash}textit\{MUTANT\} establishes a new state-of-the-art accuracy on VQA-CP with a \$10.57{\textbackslash}\%\$ improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.},
	urldate = {2020-09-24},
	journal = {arXiv:2009.08566 [cs]},
	author = {Gokhale, Tejas and Banerjee, Pratyay and Baral, Chitta and Yang, Yezhou},
	month = sep,
	year = {2020}
}

@article{lee_regularizing_2020,
	title = {Regularizing {Attention} {Networks} for {Anomaly} {Detection} in {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2009.10054},
	abstract = {For stability and reliability of real-world applications, the robustness of DNNs in unimodal tasks has been evaluated. However, few studies consider abnormal situations that a visual question answering (VQA) model might encounter at test time after deployment in the real-world. In this study, we evaluate the robustness of state-of-the-art VQA models to five different anomalies, including worst-case scenarios, the most frequent scenarios, and the current limitation of VQA models. Different from the results in unimodal tasks, the maximum confidence of answers in VQA models cannot detect anomalous inputs, and post-training of the outputs, such as outlier exposure, is ineffective for VQA models. Thus, we propose an attention-based method, which uses confidence of reasoning between input images and questions and shows much more promising results than the previous methods in unimodal tasks. In addition, we show that a maximum entropy regularization of attention networks can significantly improve the attention-based anomaly detection of the VQA models. Thanks to the simplicity, attention-based anomaly detection and the regularization are model-agnostic methods, which can be used for various cross-modal attentions in the state-of-the-art VQA models. The results imply that cross-modal attention in VQA is important to improve not only VQA accuracy, but also the robustness to various anomalies.},
	urldate = {2020-09-24},
	journal = {arXiv:2009.10054 [cs]},
	author = {Lee, Doyup and Cheon, Yeongjae and Han, Wook-Shin},
	month = sep,
	year = {2020}
}

@article{cao_interpretable_2019,
	title = {Interpretable {Visual} {Question} {Answering} by {Reasoning} on {Dependency} {Trees}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8847465/},
	doi = {10.1109/TPAMI.2019.2943456},
	abstract = {Collaborative reasoning for understanding image-question pairs is a very critical but underexplored topic in interpretable visual question answering systems. Although very recent studies have attempted to use explicit compositional processes to assemble multiple subtasks embedded in questions, their models heavily rely on annotations or handcrafted rules to obtain valid reasoning processes, which leads to either heavy workloads or poor performance on compositional reasoning. In this paper, to better align image and language domains in diverse and unrestricted cases, we propose a novel neural network model that performs global reasoning on a dependency tree parsed from the question; thus, our model is called a parse-tree-guided reasoning network (PTGRN). This network consists of three collaborative modules: i) an attention module that exploits the local visual evidence of each word parsed from the question, ii) a gated residual composition module that composes the previously mined evidence, and iii) a parse-tree-guided propagation module that passes the mined evidence along the parse tree. Thus, PTGRN is capable of building an interpretable visual question answering (VQA) system that gradually derives image cues following question-driven parse-tree reasoning. Experiments on relational datasets demonstrate the superiority of PTGRN over current state-of-the-art VQA methods, and the visualization results highlight the explainable capability of our reasoning system.},
	language = {en},
	urldate = {2020-09-24},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cao, Qingxing and Liang, Xiaodan and Li, Bailin and Lin, Liang},
	year = {2019}
}

@article{liu_adversarial_2020,
	title = {Adversarial {Learning} {With} {Multi}-{Modal} {Attention} for {Visual} {Question} {Answering}},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9174895/},
	doi = {10.1109/TNNLS.2020.3016083},
	abstract = {Visual question answering (VQA) has been proposed as a challenging task and attracted extensive research attention. It aims to learn a joint representation of the question–image pair for answer inference. Most of the existing methods focus on exploring the multi-modal correlation between the question and image to learn the joint representation. However, the answer-related information is not fully captured by these methods, which results that the learned representation is ineffective to reﬂect the answer of the question. To tackle this problem, we propose a novel model, i.e., adversarial learning with multi-modal attention (ALMA), for VQA. An adversarial learning-based framework is proposed to learn the joint representation to effectively reﬂect the answer-related information. Speciﬁcally, multi-modal attention with the Siamese similarity learning method is designed to build two embedding generators, i.e., question–image embedding and question–answer embedding. Then, adversarial learning is conducted as an interplay between the two embedding generators and an embedding discriminator. The generators have the purpose of generating two modalityinvariant representations for the question–image and question–answer pairs, whereas the embedding discriminator aims to discriminate the two representations. Both the multi-modal attention module and the adversarial networks are integrated into an end-to-end uniﬁed framework to infer the answer. Experiments performed on three benchmark data sets conﬁrm the favorable performance of ALMA compared with state-of-theart approaches.},
	language = {en},
	urldate = {2020-09-26},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Liu, Yun and Zhang, Xiaoming and Huang, Feiran and Cheng, Lei and Li, Zhoujun},
	year = {2020}
}

@article{gupta_hierarchical_2020,
	title = {Hierarchical {Deep} {Multi}-modal {Network} for {Medical} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2009.12770},
	abstract = {Visual Question Answering in Medical domain (VQA-Med) plays an important role in providing medical assistance to the end-users. These users are expected to raise either a straightforward question with a Yes/No answer or a challenging question that requires a detailed and descriptive answer. The existing techniques in VQA-Med fail to distinguish between the different question types sometimes complicates the simpler problems, or over-simplifies the complicated ones. It is certainly true that for different question types, several distinct systems can lead to confusion and discomfort for the end-users. To address this issue, we propose a hierarchical deep multi-modal network that analyzes and classifies end-user questions/queries and then incorporates a query-specific approach for answer prediction. We refer our proposed approach as Hierarchical Question Segregation based Visual Question Answering, in short HQS-VQA. Our contributions are three-fold, viz. firstly, we propose a question segregation (QS) technique for VQAMed; secondly, we integrate the QS model to the hierarchical deep multi-modal neural network to generate proper answers to the queries related to medical images; and thirdly, we study the impact of QS in Medical-VQA by comparing the performance of the proposed model with QS and a model without QS. We evaluate the performance of our proposed model on two benchmark datasets, viz. RAD and CLEF18. Experimental results show that our proposed HQS-VQA technique outperforms the baseline models with significant margins. We also conduct a detailed quantitative and qualitative analysis of the obtained results and discover potential causes of errors and their solutions.},
	urldate = {2020-10-10},
	journal = {arXiv:2009.12770 [cs]},
	author = {Gupta, Deepak and Suman, Swati and Ekbal, Asif},
	month = sep,
	year = {2020}
}

@article{farazi_attention_2020,
	title = {Attention {Guided} {Semantic} {Relationship} {Parsing} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2010.01725},
	abstract = {Humans explain inter-object relationships with semantic labels that demonstrate a high-level understanding required to perform complex Vision-Language tasks such as Visual Question Answering (VQA). However, existing VQA models represent relationships as a combination of object-level visual features which constrain a model to express interactions between objects in a single domain, while the model is trying to solve a multi-modal task. In this paper, we propose a general purpose semantic relationship parser which generates a semantic feature vector for each subject-predicate-object triplet in an image, and a Mutual and Self Attention (MSA) mechanism that learns to identify relationship triplets that are important to answer the given question. To motivate the significance of semantic relationships, we show an oracle setting with ground-truth relationship triplets, where our model achieves a {\textasciitilde}25\% accuracy gain over the closest state-of-the-art model on the challenging GQA dataset. Further, with our semantic parser, we show that our model outperforms other comparable approaches on VQA and GQA datasets.},
	urldate = {2020-10-10},
	journal = {arXiv:2010.01725 [cs]},
	author = {Farazi, Moshiur and Khan, Salman and Barnes, Nick},
	month = oct,
	year = {2020}
}

@article{han_finding_2020,
	title = {Finding the {Evidence}: {Localization}-aware {Answer} {Prediction} for {Text} {Visual} {Question} {Answering}},
	shorttitle = {Finding the {Evidence}},
	url = {http://arxiv.org/abs/2010.02582},
	abstract = {Image text carries essential information to understand the scene and perform reasoning. Text-based visual question answering (text VQA) task focuses on visual questions that require reading text in images. Existing text VQA systems generate an answer by selecting from optical character recognition (OCR) texts or a fixed vocabulary. Positional information of text is underused and there is a lack of evidence for the generated answer. As such, this paper proposes a localization-aware answer prediction network (LaAP-Net) to address this challenge. Our LaAP-Net not only generates the answer to the question but also predicts a bounding box as evidence of the generated answer. Moreover, a context-enriched OCR representation (COR) for multimodal fusion is proposed to facilitate the localization task. Our proposed LaAP-Net outperforms existing approaches on three benchmark datasets for the text VQA task by a noticeable margin.},
	urldate = {2020-10-10},
	journal = {arXiv:2010.02582 [cs]},
	author = {Han, Wei and Huang, Hantao and Han, Tao},
	month = oct,
	year = {2020}
}

@article{tang_interpretable_2020,
	title = {Interpretable {Neural} {Computation} for {Real}-{World} {Compositional} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2010.04913},
	abstract = {There are two main lines of research on visual question answering (VQA): compositional model with explicit multi-hop reasoning, and monolithic network with implicit reasoning in the latent feature space. The former excels in interpretability and compositionality but fails on real-world images, while the latter usually achieves better performance due to model flexibility and parameter efficiency. We aim to combine the two to build an interpretable framework for real-world compositional VQA. In our framework, images and questions are disentangled into scene graphs and programs, and a symbolic program executor runs on them with full transparency to select the attention regions, which are then iteratively passed to a visual-linguistic pre-trained encoder to predict answers. Experiments conducted on the GQA benchmark demonstrate that our framework outperforms the compositional prior arts and achieves competitive accuracy among monolithic ones. With respect to the validity, plausibility and distribution metrics, our framework surpasses others by a considerable margin.},
	urldate = {2020-10-21},
	journal = {arXiv:2010.04913 [cs]},
	author = {Tang, Ruixue and Ma, Chao},
	month = oct,
	year = {2020}
}

@article{kant_contrast_2020,
	title = {Contrast and {Classify}: {Alternate} {Training} for {Robust} {VQA}},
	shorttitle = {Contrast and {Classify}},
	url = {http://arxiv.org/abs/2010.06087},
	abstract = {Recent Visual Question Answering (VQA) models have shown impressive performance on the VQA benchmark but remain sensitive to small linguistic variations in input questions. Existing approaches address this by augmenting the dataset with question paraphrases from visual question generation models or adversarial perturbations. These approaches use the combined data to learn an answer classifier by minimizing the standard cross-entropy loss. To more effectively leverage the augmented data, we build on the recent success in contrastive learning. We propose a novel training paradigm (ConCAT) that alternately optimizes cross-entropy and contrastive losses. The contrastive loss encourages representations to be robust to linguistic variations in questions while the cross-entropy loss preserves the discriminative power of the representations for answer classification. We find that alternately optimizing both losses is key to effective training. VQA models trained with ConCAT achieve higher consensus scores on the VQA-Rephrasings dataset as well as higher VQA accuracy on the VQA 2.0 dataset compared to existing approaches across a variety of data augmentation strategies.},
	urldate = {2020-10-21},
	journal = {arXiv:2010.06087 [cs]},
	author = {Kant, Yash and Moudgil, Abhinav and Batra, Dhruv and Parikh, Devi and Agrawal, Harsh},
	month = oct,
	year = {2020}
}

@article{marasovic_natural_2020,
	title = {Natural {Language} {Rationales} with {Full}-{Stack} {Visual} {Reasoning}: {From} {Pixels} to {Semantic} {Frames} to {Commonsense} {Graphs}},
	shorttitle = {Natural {Language} {Rationales} with {Full}-{Stack} {Visual} {Reasoning}},
	url = {http://arxiv.org/abs/2010.07526},
	abstract = {Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale{\textasciicircum}VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks.},
	urldate = {2020-10-21},
	journal = {arXiv:2010.07526 [cs]},
	author = {Marasović, Ana and Bhagavatula, Chandra and Park, Jae Sung and Bras, Ronan Le and Smith, Noah A. and Choi, Yejin},
	month = oct,
	year = {2020}
}

@article{huang_answer-checking_2020,
	title = {Answer-checking in {Context}: {A} {Multi}-modal {FullyAttention} {Network} for {Visual} {Question} {Answering}},
	shorttitle = {Answer-checking in {Context}},
	url = {http://arxiv.org/abs/2010.08708},
	abstract = {Visual Question Answering (VQA) is challenging due to the complex cross-modal relations. It has received extensive attention from the research community. From the human perspective, to answer a visual question, one needs to read the question and then refer to the image to generate an answer. This answer will then be checked against the question and image again for the final confirmation. In this paper, we mimic this process and propose a fully attention based VQA architecture. Moreover, an answer-checking module is proposed to perform a unified attention on the jointly answer, question and image representation to update the answer. This mimics the human answer checking process to consider the answer in the context. With answer-checking modules and transferred BERT layers, our model achieves the state-of-the-art accuracy 71.57{\textbackslash}\% using fewer parameters on VQA-v2.0 test-standard split.},
	urldate = {2020-10-21},
	journal = {arXiv:2010.08708 [cs]},
	author = {Huang, Hantao and Han, Tao and Han, Wei and Yap, Deep and Chiang, Cheng-Ming},
	month = oct,
	year = {2020}
}

@article{dharur_sort-ing_2020,
	title = {{SOrT}-ing {VQA} {Models} : {Contrastive} {Gradient} {Learning} for {Improved} {Consistency}},
	shorttitle = {{SOrT}-ing {VQA} {Models}},
	url = {http://arxiv.org/abs/2010.10038},
	abstract = {Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world -- they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the higher level question correctly. To address this, we first present a gradient-based interpretability approach to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach called Sub-question Oriented Tuning (SOrT) which encourages models to rank relevant sub-questions higher than irrelevant questions for an {\textless}\$image, reasoning-question\${\textgreater} pair. We show that SOrT improves model consistency by upto 6.5\% points over existing baselines, while also improving visual grounding.},
	urldate = {2020-10-21},
	journal = {arXiv:2010.10038 [cs]},
	author = {Dharur, Sameer and Tendulkar, Purva and Batra, Dhruv and Parikh, Devi and Selvaraju, Ramprasaath R.},
	month = oct,
	year = {2020}
}


@article{chen_characterizing_2020,
	title = {Characterizing {Datasets} for {Social} {Visual} {Question} {Answering}, and the {New} {TinySocial} {Dataset}},
	url = {http://arxiv.org/abs/2010.11997},
	abstract = {Modern social intelligence includes the ability to watch videos and answer questions about social and theory-of-mind-related content, e.g., for a scene in Harry Potter, "Is the father really upset about the boys flying the car?" Social visual question answering (social VQA) is emerging as a valuable methodology for studying social reasoning in both humans (e.g., children with autism) and AI agents. However, this problem space spans enormous variations in both videos and questions. We discuss methods for creating and characterizing social VQA datasets, including 1) crowdsourcing versus in-house authoring, including sample comparisons of two new datasets that we created (TinySocial-Crowd and TinySocial-InHouse) and the previously existing Social-IQ dataset; 2) a new rubric for characterizing the difficulty and content of a given video; and 3) a new rubric for characterizing question types. We close by describing how having well-characterized social VQA datasets will enhance the explainability of AI agents and can also inform assessments and educational interventions for people.},
	urldate = {2020-11-11},
	journal = {arXiv:2010.11997 [cs]},
	author = {Chen, Zhanwen and Li, Shiyao and Rashedi, Roxanne and Zi, Xiaoman and Elrod-Erickson, Morgan and Hollis, Bryan and Maliakal, Angela and Shen, Xinyu and Zhao, Simeng and Kunda, Maithilee},
	month = oct,
	year = {2020}
}

@article{he_pathological_2020,
	title = {Pathological {Visual} {Question} {Answering}},
	url = {https://arxiv.org/abs/2010.12435v1},
	author = {He, Xuehai and Cai, Zhuo and Wei, Wenlan and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
	month = oct,
	year = {2020}
}

@article{dua_beyond_2020,
	title = {Beyond {VQA}: {Generating} {Multi}-word {Answer} and {Rationale} to {Visual} {Questions}},
	shorttitle = {Beyond {VQA}},
	url = {https://arxiv.org/abs/2010.12852v1},
	author = {Dua, Radhika and Kancheti, Sai Srinivas and Balasubramanian, Vineeth N.},
	month = oct,
	year = {2020}
}

@article{khan_mmft-bert_2020,
	title = {{MMFT}-{BERT}: {Multimodal} {Fusion} {Transformer} with {BERT} {Encodings} for {Visual} {Question} {Answering}},
	shorttitle = {{MMFT}-{BERT}},
	url = {http://arxiv.org/abs/2010.14095},
	journal = {arXiv:2010.14095 [cs]},
	author = {Khan, Aisha Urooj and Mazaheri, Amir and Lobo, Niels da Vitoria and Shah, Mubarak},
	month = oct,
	year = {2020}
}

@article{frolov_leveraging_2020,
	title = {Leveraging {Visual} {Question} {Answering} to {Improve} {Text}-to-{Image} {Synthesis}},
	url = {http://arxiv.org/abs/2010.14953},
	journal = {arXiv:2010.14953 [cs]},
	author = {Frolov, Stanislav and Jolly, Shailza and Hees, Jörn and Dengel, Andreas},
	month = oct,
	year = {2020}
}

@article{guo_loss-rescaling_2020,
	title = {Loss-rescaling {VQA}: {Revisiting} {Language} {Prior} {Problem} from a {Class}-imbalance {View}},
	shorttitle = {Loss-rescaling {VQA}},
	url = {http://arxiv.org/abs/2010.16010},
	journal = {arXiv:2010.16010 [cs]},
	author = {Guo, Yangyang and Nie, Liqiang and Cheng, Zhiyong and Tian, Qi},
	month = oct,
	year = {2020}
}

@article{rahman_improved_2020,
	title = {An {Improved} {Attention} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2011.02164},
	journal = {arXiv:2011.02164 [cs]},
	author = {Rahman, Tanzila and Chou, Shih-Han and Sigal, Leonid and Carenini, Giuseppe},
	month = nov,
	year = {2020}
}


@article{le-ngo_logically_2020,
	title = {Logically {Consistent} {Loss} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2011.10094},
	journal = {arXiv:2011.10094 [cs]},
	author = {Le-Ngo, Anh-Cat and Tran, Truyen and Rana, Santu and Gupta, Sunil and Venkatesh, Svetha},
	month = nov,
	year = {2020}
}

@article{liang_lrta_2020,
	title = {{LRTA}: {A} {Transparent} {Neural}-{Symbolic} {Reasoning} {Framework} with {Modular} {Supervision} for {Visual} {Question} {Answering}},
	shorttitle = {{LRTA}},
	url = {http://arxiv.org/abs/2011.10731},
	journal = {arXiv:2011.10731 [cs]},
	author = {Liang, Weixin and Niu, Feiyang and Reganti, Aishwarya and Thattai, Govind and Tur, Gokhan},
	month = nov,
	year = {2020}
}

@article{wang_interpretable_2020,
	title = {Interpretable {Visual} {Reasoning} via {Induced} {Symbolic} {Space}},
	url = {http://arxiv.org/abs/2011.11603},
	journal = {arXiv:2011.11603 [cs]},
	author = {Wang, Zhonghao and Yu, Mo and Wang, Kai and Xiong, Jinjun and Hwu, Wen-mei and Hasegawa-Johnson, Mark and Shi, Humphrey},
	month = nov,
	year = {2020}
}


@article{ma_xtqa_2020,
	title = {{XTQA}: {Span}-{Level} {Explanations} of the {Textbook} {Question} {Answering}},
	shorttitle = {{XTQA}},
	url = {http://arxiv.org/abs/2011.12662}
	urldate = {2020-12-04},
	journal = {arXiv:2011.12662 [cs]},
	author = {Ma, Jie and Liu, Jun and Li, Junjun and Zheng, Qinghua and Yin, Qingyu and Zhou, Jianlong and Huang, Yi},
	month = nov,
	year = {2020}
}

@article{mani_point_2020,
	title = {Point and {Ask}: {Incorporating} {Pointing} into {Visual} {Question} {Answering}},
	shorttitle = {Point and {Ask}},
	url = {http://arxiv.org/abs/2011.13681}
	urldate = {2020-12-04},
	journal = {arXiv:2011.13681 [cs]},
	author = {Mani, Arjun and Hinthorn, Will and Yoo, Nobline and Russakovsky, Olga},
	month = nov,
	year = {2020}
}

@article{whitehead_learning_2020,
	title = {Learning from {Lexical} {Perturbations} for {Consistent} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2011.13406}
	urldate = {2020-12-04},
	journal = {arXiv:2011.13406 [cs]},
	author = {Whitehead, Spencer and Wu, Hui and Fung, Yi Ren and Ji, Heng and Feris, Rogerio and Saenko, Kate},
	month = nov,
	year = {2020}
}

@article{hong_transformation_2020,
	title = {Transformation {Driven} {Visual} {Reasoning}},
	url = {http://arxiv.org/abs/2011.13160}
	urldate = {2020-12-04},
	journal = {arXiv:2011.13160 [cs]},
	author = {Hong, Xin and Lan, Yanyan and Pang, Liang and Guo, Jiafeng and Cheng, Xueqi},
	month = nov,
	year = {2020}
}

@article{bugliarello_multimodal_2020,
	title = {Multimodal {Pretraining} {Unmasked}: {Unifying} the {Vision} and {Language} {BERTs}},
	shorttitle = {Multimodal {Pretraining} {Unmasked}},
	url = {http://arxiv.org/abs/2011.15124}
	urldate = {2020-12-04},
	journal = {arXiv:2011.15124 [cs]},
	author = {Bugliarello, Emanuele and Cotterell, Ryan and Okazaki, Naoaki and Elliott, Desmond},
	month = nov,
	year = {2020}
}


@article{banerjee_self-supervised_2020,
	title = {Self-{Supervised} {VQA}: {Answering} {Visual} {Questions} using {Images} and {Captions}},
	shorttitle = {Self-{Supervised} {VQA}},
	url = {http://arxiv.org/abs/2012.02356},
	journal = {arXiv:2012.02356 [cs]},
	author = {Banerjee, Pratyay and Gokhale, Tejas and Yang, Yezhou and Baral, Chitta},
	month = dec,
	year = {2020}
}

@article{patel_generating_2020,
	title = {Generating {Natural} {Questions} from {Images} for {Multimodal} {Assistants}},
	url = {http://arxiv.org/abs/2012.03678},
	journal = {arXiv:2012.03678 [cs]},
	author = {Patel, Alkesh and Bindal, Akanksha and Kotek, Hadas and Klein, Christopher and Williams, Jason},
	month = nov,
	year = {2020}
}

@article{yang_tap_2020,
	title = {{TAP}: {Text}-{Aware} {Pre}-training for {Text}-{VQA} and {Text}-{Caption}},
	shorttitle = {{TAP}},
	url = {http://arxiv.org/abs/2012.04638}
	journal = {arXiv:2012.04638 [cs]},
	author = {Yang, Zhengyuan and Lu, Yijuan and Wang, Jianfeng and Yin, Xi and Florencio, Dinei and Wang, Lijuan and Zhang, Cha and Zhang, Lei and Luo, Jiebo},
	month = dec,
	year = {2020}
}

@article{zhu_simple_2020,
	title = {Simple is not {Easy}: {A} {Simple} {Strong} {Baseline} for {TextVQA} and {TextCaps}},
	shorttitle = {Simple is not {Easy}},
	url = {http://arxiv.org/abs/2012.05153},
	journal = {arXiv:2012.05153 [cs]},
	author = {Zhu, Qi and Gao, Chenyu and Wang, Peng and Wu, Qi},
	month = dec,
	year = {2020}
}


@article{song_kvl-bert_2020,
	title = {{KVL}-{BERT}: {Knowledge} {Enhanced} {Visual}-and-{Linguistic} {BERT} for {Visual} {Commonsense} {Reasoning}},
	shorttitle = {{KVL}-{BERT}},
	url = {http://arxiv.org/abs/2012.07000},
	journal = {arXiv:2012.07000 [cs]},
	author = {Song, Dandan and Ma, Siyi and Sun, Zhanchen and Yang, Sicheng and Liao, Lejian},
	month = dec,
	year = {2020}
}

@article{cao_knowledge-routed_2020,
	title = {Knowledge-{Routed} {Visual} {Question} {Reasoning}: {Challenges} for {Deep} {Representation} {Embedding}},
	shorttitle = {Knowledge-{Routed} {Visual} {Question} {Reasoning}},
	urldate = {2020-12-18},
	journal = {arXiv:2012.07192 [cs]},
	author = {Cao, Qingxing and Li, Bailin and Liang, Xiaodan and Wang, Keze and Lin, Liang},
	month = dec,
	year = {2020}
}

@article{wang_minivlm_2020,
	title = {{MiniVLM}: {A} {Smaller} and {Faster} {Vision}-{Language} {Model}},
	shorttitle = {{MiniVLM}},
	urldate = {2020-12-18},
	journal = {arXiv:2012.06946 [cs]},
	author = {Wang, Jianfeng and Hu, Xiaowei and Zhang, Pengchuan and Li, Xiujun and Wang, Lijuan and Zhang, Lei and Gao, Jianfeng and Liu, Zicheng},
	month = dec,
	year = {2020}
}

@article{li_closer_2020,
	title = {A {Closer} {Look} at the {Robustness} of {Vision}-and-{Language} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2012.08673},
	journal = {arXiv:2012.08673 [cs]},
	author = {Li, Linjie and Gan, Zhe and Liu, Jingjing},
	month = dec,
	year = {2020}
}


@article{uppal_multimodal_2020,
	title = {Multimodal {Research} in {Vision} and {Language}: {A} {Review} of {Current} and {Emerging} {Trends}},
	shorttitle = {Multimodal {Research} in {Vision} and {Language}},
	url = {https://arxiv.org/abs/2010.09522v2},
	language = {en},
	journal = {arxiv}
	urldate = {2020-12-26},
	author = {Uppal, Shagun and Bhagat, Sarthak and Hazarika, Devamanyu and Majumdar, Navonil and Poria, Soujanya and Zimmermann, Roger and Zadeh, Amir},
	month = oct,
	year = {2020}
}

@inproceedings{garderes_conceptbert_2020,
	address = {Online},
	title = {{ConceptBert}: {Concept}-{Aware} {Representation} for {Visual} {Question} {Answering}},
	shorttitle = {{ConceptBert}},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.44},
	doi = {10.18653/v1/2020.findings-emnlp.44}
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP}},
	publisher = {Association for Computational Linguistics},
	author = {Gardères, François and Ziaeefard, Maryam and Abeloos, Baptiste and Lecue, Freddy},
	month = nov,
	year = {2020},
	pages = {489--498}
}

@inproceedings{chen_counterfactual_2020,
	address = {Seattle, WA, USA},
	title = {Counterfactual {Samples} {Synthesizing} for {Robust} {Visual} {Question} {Answering}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157377/},
	doi = {10.1109/CVPR42600.2020.01081},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Long and Yan, Xin and Xiao, Jun and Zhang, Hanwang and Pu, Shiliang and Zhuang, Yueting},
	month = jun,
	year = {2020}
}

@article{winterbottom_modality_2020,
	title = {On {Modality} {Bias} in the {TVQA} {Dataset}},
	url = {http://arxiv.org/abs/2012.10210}
	journal = {BMVC},
	author = {Winterbottom, Thomas and Xiao, Sarah and McLean, Alistair and Moubayed, Noura Al},
	month = dec,
	year = {2020}
}

@article{marino_krisp_2020,
	title = {{KRISP}: {Integrating} {Implicit} and {Symbolic} {Knowledge} for {Open}-{Domain} {Knowledge}-{Based} {VQA}},
	shorttitle = {{KRISP}},
	url = {http://arxiv.org/abs/2012.11014}
	journal = {arXiv:2012.11014 [cs]},
	author = {Marino, Kenneth and Chen, Xinlei and Parikh, Devi and Gupta, Abhinav and Rohrbach, Marcus},
	month = dec,
	year = {2020}
}

@article{yang_object-centric_2020,
	title = {Object-{Centric} {Diagnosis} of {Visual} {Reasoning}},
	url = {http://arxiv.org/abs/2012.11587}
	journal = {arXiv:2012.11587 [cs]},
	author = {Yang, Jianwei and Mao, Jiayuan and Wu, Jiajun and Parikh, Devi and Cox, David D. and Tenenbaum, Joshua B. and Gan, Chuang},
	month = dec,
	year = {2020}
}

@article{zhu_overcoming_2020,
	title = {Overcoming {Language} {Priors} with {Self}-supervised {Learning} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2012.11528}
	journal = {IJCAI},
	author = {Zhu, Xi and Mao, Zhendong and Liu, Chunxiao and Zhang, Peng and Wang, Bin and Zhang, Yongdong},
	month = dec,
	year = {2020}
}

@article{dognin_image_2020,
	title = {Image {Captioning} as an {Assistive} {Technology}: {Lessons} {Learned} from {VizWiz} 2020 {Challenge}},
	shorttitle = {Image {Captioning} as an {Assistive} {Technology}},
	url = {http://arxiv.org/abs/2012.11696}
	journal = {arXiv:2012.11696 [cs]},
	author = {Dognin, Pierre and Melnyk, Igor and Mroueh, Youssef and Padhi, Inkit and Rigotti, Mattia and Ross, Jarret and Schiff, Yair and Young, Richard A. and Belgodere, Brian},
	month = dec,
	year = {2020}
}

@article{parcalabescu_seeing_2020,
	title = {Seeing past words: {Testing} the cross-modal capabilities of pretrained {V}\&{L} models},
	shorttitle = {Seeing past words},
	url = {http://arxiv.org/abs/2012.12352}
	journal = {arXiv:2012.12352 [cs]},
	author = {Parcalabescu, Letitia and Gatt, Albert and Frank, Anette and Calixto, Iacer},
	month = dec,
	year = {2020}
}

@article{li_unimo_2020,
	title = {{UNIMO}: {Towards} {Unified}-{Modal} {Understanding} and {Generation} via {Cross}-{Modal} {Contrastive} {Learning}},
	shorttitle = {{UNIMO}},
	url = {http://arxiv.org/abs/2012.15409},
	journal = {arXiv:2012.15409 [cs]},
	author = {Li, Wei and Gao, Can and Niu, Guocheng and Xiao, Xinyan and Liu, Hao and Liu, Jiachen and Wu, Hua and Wang, Haifeng},
	month = dec,
	year = {2020}
}

@article{zhang_vinvl_2021,
	title = {{VinVL}: {Making} {Visual} {Representations} {Matter} in {Vision}-{Language} {Models}},
	shorttitle = {{VinVL}},
	url = {http://arxiv.org/abs/2101.00529},
	journal = {arXiv:2101.00529 [cs]},
	author = {Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
	month = jan,
	year = {2021}
}


@article{damodaran_understanding_2021,
	title = {Understanding the {Role} of {Scene} {Graphs} in {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2101.05479},
	journal = {arXiv:2101.05479 [cs]},
	author = {Damodaran, Vinay and Chakravarthy, Sharanya and Kumar, Akshay and Umapathy, Anjana and Mitamura, Teruko and Nakashima, Yuta and Garcia, Noa and Chu, Chenhui},
	month = jan,
	year = {2021}
}

@article{patel_recent_2021,
	title = {Recent {Advances} in {Video} {Question} {Answering}: {A} {Review} of {Datasets} and {Methods}},
	shorttitle = {Recent {Advances} in {Video} {Question} {Answering}},
	url = {http://arxiv.org/abs/2101.05954},
	journal = {arXiv:2101.05954 [cs]},
	author = {Patel, Devshree and Parikh, Ratnam and Shastri, Yesha},
	month = jan,
	year = {2021}
}

@article{shevchenko_reasoning_2021,
	title = {Reasoning over {Vision} and {Language}: {Exploring} the {Benefits} of {Supplemental} {Knowledge}},
	shorttitle = {Reasoning over {Vision} and {Language}},
	url = {http://arxiv.org/abs/2101.06013},
	journal = {arXiv:2101.06013 [cs]},
	author = {Shevchenko, Violetta and Teney, Damien and Dick, Anthony and Hengel, Anton van den},
	month = jan,
	year = {2021}
}

@article{wang_latent_2021,
	title = {Latent {Variable} {Models} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2101.06399},
	journal = {arXiv:2101.06399 [cs]},
	author = {Wang, Zixu and Miao, Yishu and Specia, Lucia},
	month = jan,
	year = {2021}
}


@article{kim_visual_2021,
	title = {Visual {Question} {Answering} based on {Local}-{Scene}-{Aware} {Referring} {Expression} {Generation}},
	url = {http://arxiv.org/abs/2101.08978},
	journal = {arXiv:2101.08978 [cs]},
	author = {Kim, Jung-Jun and Lee, Dong-Gyu and Wu, Jialin and Jung, Hong-Gyu and Lee, Seong-Whan},
	month = jan,
	year = {2021}
}

@article{liu_answer_2021,
	title = {Answer {Questions} with {Right} {Image} {Regions}: {A} {Visual} {Attention} {Regularization} {Approach}},
	shorttitle = {Answer {Questions} with {Right} {Image} {Regions}},
	url = {http://arxiv.org/abs/2102.01916},
	journal = {arXiv:2102.01916 [cs]},
	author = {Liu, Yibing and Guo, Yangyang and Yin, Jianhua and Song, Xuemeng and Liu, Weifeng and Nie, Liqiang},
	month = feb,
	year = {2021}
}
