
@article{azuma_scanqa_2021,
	title = {{ScanQA}: {3D} {Question} {Answering} for {Spatial} {Scene} {Understanding}},
	shorttitle = {{ScanQA}},
	url = {http://arxiv.org/abs/2112.10482},
	abstract = {We propose a new 3D spatial understanding task of 3D Question Answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of the rich RGB-D indoor scan and answer the given textual questions about the 3D scene. Unlike the 2D-question answering of VQA, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail the object localization from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model, where the model learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates the language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine described objects in textual questions. We collected human-edited question-answer pairs with free-form answers that are grounded to 3D objects in each 3D scene. Our new ScanQA dataset contains over 41K question-answer pairs from the 800 indoor scenes drawn from the ScanNet dataset. To the best of our knowledge, ScanQA is the first large-scale effort to perform object-grounded question-answering in 3D environments.},
	urldate = {2021-12-25},
	journal = {arXiv:2112.10482 [cs]},
	author = {Azuma, Daichi and Miyanishi, Taiki and Kurita, Shuhei and Kawanabe, Motoki},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.10482
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, bib},
	annote = {Comment: 11 pages for the main paper and 9 pages for the supplementary material. 10 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\KB9VGEVK\\2112.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\85E638R4\\Azuma 等。 - 2021 - ScanQA 3D Question Answering for Spatial Scene Un.pdf:application/pdf},
}

@article{biten_latr_2021,
	title = {{LaTr}: {Layout}-{Aware} {Transformer} for {Scene}-{Text} {VQA}},
	shorttitle = {{LaTr}},
	url = {http://arxiv.org/abs/2112.12494},
	abstract = {We propose a novel multimodal architecture for Scene Text Visual Question Answering (STVQA), named Layout-Aware Transformer (LaTr). The task of STVQA requires models to reason over different modalities. Thus, we first investigate the impact of each modality, and reveal the importance of the language module, especially when enriched with layout information. Accounting for this, we propose a single objective pre-training scheme that requires only text and spatial cues. We show that applying this pre-training scheme on scanned documents has certain advantages over using natural images, despite the domain gap. Scanned documents are easy to procure, text-dense and have a variety of layouts, helping the model learn various spatial cues (e.g. left-of, below etc.) by tying together language and layout information. Compared to existing approaches, our method performs vocabulary-free decoding and, as shown, generalizes well beyond the training vocabulary. We further demonstrate that LaTr improves robustness towards OCR errors, a common reason for failure cases in STVQA. In addition, by leveraging a vision transformer, we eliminate the need for an external object detector. LaTr outperforms state-of-the-art STVQA methods on multiple datasets. In particular, +7.6\% on TextVQA, +10.8\% on ST-VQA and +4.0\% on OCR-VQA (all absolute accuracy numbers).},
	urldate = {2021-12-25},
	journal = {arXiv:2112.12494 [cs]},
	author = {Biten, Ali Furkan and Litman, Ron and Xie, Yusheng and Appalaraju, Srikar and Manmatha, R.},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.12494
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, bib},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\8ZVQ5TIA\\2112.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\RBNADUQZ\\Biten 等。 - 2021 - LaTr Layout-Aware Transformer for Scene-Text VQA.pdf:application/pdf},
}

@article{banerjee_weaqa_2021,
	title = {{WeaQA}: {Weak} {Supervision} via {Captions} for {Visual} {Question} {Answering}},
	shorttitle = {{WeaQA}},
	url = {http://arxiv.org/abs/2012.02356},
	abstract = {Methodologies for training visual question answering (VQA) models assume the availability of datasets with human-annotated {\textbackslash}textit\{Image-Question-Answer\} (I-Q-A) triplets. This has led to heavy reliance on datasets and a lack of generalization to new types of questions and scenes. Linguistic priors along with biases and errors due to annotator subjectivity have been shown to percolate into VQA models trained on such samples. We study whether models can be trained without any human-annotated Q-A pairs, but only with images and their associated textual descriptions or captions. We present a method to train models with synthetic Q-A pairs generated procedurally from captions. Additionally, we demonstrate the efficacy of spatial-pyramid image patches as a simple but effective alternative to dense and costly object bounding box annotations used in existing VQA models. Our experiments on three VQA benchmarks demonstrate the efficacy of this weakly-supervised approach, especially on the VQA-CP challenge, which tests performance under changing linguistic priors.},
	urldate = {2021-12-25},
	journal = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
	author = {Banerjee, Pratyay and Gokhale, Tejas and Yang, Yezhou and Baral, Chitta},
	month = may,
	year = {2021},
	note = {arXiv: 2012.02356},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, bib},
	annote = {Comment: Accepted in Findings of ACL 2021},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\CUWTQTKY\\2012.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\8X9PWTYZ\\Banerjee 等。 - 2021 - WeaQA Weak Supervision via Captions for Visual Qu.pdf:application/pdf},
}

@article{song_kvl-bert_2020,
	title = {{KVL}-{BERT}: {Knowledge} {Enhanced} {Visual}-and-{Linguistic} {BERT} for {Visual} {Commonsense} {Reasoning}},
	shorttitle = {{KVL}-{BERT}},
	url = {http://arxiv.org/abs/2012.07000},
	abstract = {Reasoning is a critical ability towards complete visual understanding. To develop machine with cognition-level visual understanding and reasoning abilities, the visual commonsense reasoning (VCR) task has been introduced. In VCR, given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. The methods adopting the powerful BERT model as the backbone for learning joint representation of image content and natural language have shown promising improvements on VCR. However, none of the existing methods have utilized commonsense knowledge in visual commonsense reasoning, which we believe will be greatly helpful in this task. With the support of commonsense knowledge, complex questions even if the required information is not depicted in the image can be answered with cognitive reasoning. Therefore, we incorporate commonsense knowledge into the cross-modal BERT, and propose a novel Knowledge Enhanced Visual-and-Linguistic BERT (KVL-BERT for short) model. Besides taking visual and linguistic contents as input, external commonsense knowledge extracted from ConceptNet is integrated into the multi-layer Transformer. In order to reserve the structural information and semantic representation of the original sentence, we propose using relative position embedding and mask-self-attention to weaken the effect between the injected commonsense knowledge and other unrelated components in the input sequence. Compared to other task-specific models and general task-agnostic pre-training models, our KVL-BERT outperforms them by a large margin.},
	urldate = {2021-12-25},
	journal = {arXiv:2012.07000 [cs]},
	author = {Song, Dandan and Ma, Siyi and Sun, Zhanchen and Yang, Sicheng and Liao, Lejian},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.07000},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, bib},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\J6VHDC5R\\2012.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\757FKCIV\\Song 等。 - 2020 - KVL-BERT Knowledge Enhanced Visual-and-Linguistic.pdf:application/pdf},
}

@article{cao_knowledge-routed_2020,
	title = {Knowledge-{Routed} {Visual} {Question} {Reasoning}: {Challenges} for {Deep} {Representation} {Embedding}},
	shorttitle = {Knowledge-{Routed} {Visual} {Question} {Reasoning}},
	url = {http://arxiv.org/abs/2012.07192},
	abstract = {Though beneficial for encouraging the Visual Question Answering (VQA) models to discover the underlying knowledge by exploiting the input-output correlation beyond image and text contexts, the existing knowledge VQA datasets are mostly annotated in a crowdsource way, e.g., collecting questions and external reasons from different users via the internet. In addition to the challenge of knowledge reasoning, how to deal with the annotator bias also remains unsolved, which often leads to superficial over-fitted correlations between questions and answers. To address this issue, we propose a novel dataset named Knowledge-Routed Visual Question Reasoning for VQA model evaluation. Considering that a desirable VQA model should correctly perceive the image context, understand the question, and incorporate its learned knowledge, our proposed dataset aims to cutoff the shortcut learning exploited by the current deep embedding models and push the research boundary of the knowledge-based visual question reasoning. Specifically, we generate the question-answer pair based on both the Visual Genome scene graph and an external knowledge base with controlled programs to disentangle the knowledge from other biases. The programs can select one or two triplets from the scene graph or knowledge base to push multi-step reasoning, avoid answer ambiguity, and balanced the answer distribution. In contrast to the existing VQA datasets, we further imply the following two major constraints on the programs to incorporate knowledge reasoning: i) multiple knowledge triplets can be related to the question, but only one knowledge relates to the image object. This can enforce the VQA model to correctly perceive the image instead of guessing the knowledge based on the given question solely; ii) all questions are based on different knowledge, but the candidate answers are the same for both the training and test sets.},
	urldate = {2021-12-25},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Cao, Qingxing and Li, Bailin and Liang, Xiaodan and Wang, Keze and Lin, Liang},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.07192},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, bib},
	annote = {Comment: To appear in TNNLS 2021. Considering that a desirable VQA model should correctly perceive the image context, understand the question, and incorporate its learned knowledge, our proposed dataset aims to cutoff the shortcut learning exploited by the current deep embedding models and push the research boundary of the knowledge-based visual question reasoning},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\EGCUIYX8\\2012.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\UKF5AGHK\\Cao 等。 - 2020 - Knowledge-Routed Visual Question Reasoning Challe.pdf:application/pdf},
}

@article{yang_learning_2020,
	title = {Learning content and context with language bias for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2012.11134},
	abstract = {Visual Question Answering (VQA) is a challenging multimodal task to answer questions about an image. Many works concentrate on how to reduce language bias which makes models answer questions ignoring visual content and language context. However, reducing language bias also weakens the ability of VQA models to learn context prior. To address this issue, we propose a novel learning strategy named CCB, which forces VQA models to answer questions relying on Content and Context with language Bias. Specifically, CCB establishes Content and Context branches on top of a base VQA model and forces them to focus on local key content and global effective context respectively. Moreover, a joint loss function is proposed to reduce the importance of biased samples and retain their beneficial influence on answering questions. Experiments show that CCB outperforms the state-of-the-art methods in terms of accuracy on VQA-CP v2.},
	urldate = {2021-12-25},
	journal = {2021 IEEE International Conference on Multimedia and Expo (ICME)},
	author = {Yang, Chao and Feng, Su and Li, Dongsheng and Shen, Huawei and Wang, Guoqing and Jiang, Bin},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.11134},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, bib},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\UERQQVGD\\2012.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\NT7JRPEB\\Yang 等。 - 2020 - Learning content and context with language bias fo.pdf:application/pdf},
}

@article{zhu_overcoming_2020,
	title = {Overcoming {Language} {Priors} with {Self}-supervised {Learning} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2012.11528},
	abstract = {Most Visual Question Answering (VQA) models suffer from the language prior problem, which is caused by inherent data biases. Specifically, VQA models tend to answer questions (e.g., what color is the banana?) based on the high-frequency answers (e.g., yellow) ignoring image contents. Existing approaches tackle this problem by creating delicate models or introducing additional visual annotations to reduce question dependency while strengthening image dependency. However, they are still subject to the language prior problem since the data biases have not been even alleviated. In this paper, we introduce a self-supervised learning framework to solve this problem. Concretely, we first automatically generate labeled data to balance the biased data, and propose a self-supervised auxiliary task to utilize the balanced data to assist the base VQA model to overcome language priors. Our method can compensate for the data biases by generating balanced data without introducing external annotations. Experimental results show that our method can significantly outperform the state-of-the-art, improving the overall accuracy from 49.50\% to 57.59\% on the most commonly used benchmark VQA-CP v2. In other words, we can increase the performance of annotation-based methods by 16\% without using external annotations.},
	urldate = {2021-12-25},
	journal = {arXiv:2012.11528 [cs]},
	author = {Zhu, Xi and Mao, Zhendong and Liu, Chunxiao and Zhang, Peng and Wang, Bin and Zhang, Yongdong},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.11528},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia, bib},
	annote = {Comment: Accepted by IJCAI 2020},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\86RQY8MK\\2012.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\AVS9UMN6\\Zhu 等。 - 2020 - Overcoming Language Priors with Self-supervised Le.pdf:application/pdf},
}

@article{whitehead_learning_2020,
	title = {Learning from {Lexical} {Perturbations} for {Consistent} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2011.13406},
	abstract = {Existing Visual Question Answering (VQA) models are often fragile and sensitive to input variations. In this paper, we propose a novel approach to address this issue based on modular networks, which creates two questions related by linguistic perturbations and regularizes the visual reasoning process between them to be consistent during training. We show that our framework markedly improves consistency and generalization ability, demonstrating the value of controlled linguistic perturbations as a useful and currently underutilized training and regularization tool for VQA models. We also present VQA Perturbed Pairings (VQA P2), a new, low-cost benchmark and augmentation pipeline to create controllable linguistic variations of VQA questions. Our benchmark uniquely draws from large-scale linguistic resources, avoiding human annotation effort while maintaining data quality compared to generative approaches. We benchmark existing VQA models using VQA P2 and provide robustness analysis on each type of linguistic variation.},
	urldate = {2021-12-25},
	journal = {arXiv:2011.13406 [cs]},
	author = {Whitehead, Spencer and Wu, Hui and Fung, Yi Ren and Ji, Heng and Feris, Rogerio and Saenko, Kate},
	month = dec,
	year = {2020},
	note = {arXiv: 2011.13406},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, bib},
	annote = {Comment: 14 pages, 8 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\X96TPF36\\2011.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\PJ8EP3FW\\Whitehead 等。 - 2020 - Learning from Lexical Perturbations for Consistent.pdf:application/pdf},
}

@article{damodaran_understanding_2021,
	title = {Understanding the {Role} of {Scene} {Graphs} in {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2101.05479},
	abstract = {Visual Question Answering (VQA) is of tremendous interest to the research community with important applications such as aiding visually impaired users and image-based search. In this work, we explore the use of scene graphs for solving the VQA task. We conduct experiments on the GQA dataset which presents a challenging set of questions requiring counting, compositionality and advanced reasoning capability, and provides scene graphs for a large number of images. We adopt image + question architectures for use with scene graphs, evaluate various scene graph generation techniques for unseen images, propose a training curriculum to leverage human-annotated and auto-generated scene graphs, and build late fusion architectures to learn from multiple image representations. We present a multi-faceted study into the use of scene graphs for VQA, making this work the first of its kind.},
	urldate = {2021-12-25},
	journal = {arXiv:2101.05479 [cs]},
	author = {Damodaran, Vinay and Chakravarthy, Sharanya and Kumar, Akshay and Umapathy, Anjana and Mitamura, Teruko and Nakashima, Yuta and Garcia, Noa and Chu, Chenhui},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.05479},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, bib},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\HSQD5352\\2101.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\TATP736L\\Damodaran 等。 - 2021 - Understanding the Role of Scene Graphs in Visual Q.pdf:application/pdf},
}

@article{barra_visual_2021,
	title = {Visual {Question} {Answering}: which investigated applications?},
	shorttitle = {Visual {Question} {Answering}},
	url = {http://arxiv.org/abs/2103.02937},
	abstract = {Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal-related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research.},
	urldate = {2021-12-25},
	journal = {arXiv:2103.02937 [cs]},
	author = {Barra, Silvio and Bisogni, Carmen and De Marsico, Maria and Ricciardi, Stefano},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.02937},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, bib},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\WU3DH8CQ\\2103.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\H5TKQUKX\\Barra 等。 - 2021 - Visual Question Answering which investigated appl.pdf:application/pdf},
}

@article{berlot-attwell_neuro-symbolic_2021,
	title = {Neuro-{Symbolic} {VQA}: {A} review from the perspective of {AGI} desiderata},
	shorttitle = {Neuro-{Symbolic} {VQA}},
	url = {https://arxiv.org/abs/2104.06365v1},
	abstract = {An ultimate goal of the AI and ML fields is artificial general intelligence (AGI); although such systems remain science fiction, various models exhibit aspects of AGI. In this work, we look at neuro-symbolic (NS)approaches to visual question answering (VQA) from the perspective of AGI desiderata. We see how well these systems meet these desiderata, and how the desiderata often pull the scientist in opposing directions. It is my hope that through this work we can temper model evaluation on benchmarks with a discussion of the properties of these systems and their potential for future extension.},
	language = {en},
	urldate = {2021-12-25},
	author = {Berlot-Attwell, Ian},
	month = apr,
	year = {2021},
	keywords = {bib},
	file = {Full Text PDF:C\:\\Users\\Taacc\\Zotero\\storage\\D5PKCJEP\\Berlot-Attwell - 2021 - Neuro-Symbolic VQA A review from the perspective .pdf:application/pdf;Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\98IE2CB3\\2104.html:text/html},
}

@article{dancette_beyond_2021,
	title = {Beyond {Question}-{Based} {Biases}: {Assessing} {Multimodal} {Shortcut} {Learning} in {Visual} {Question} {Answering}},
	shorttitle = {Beyond {Question}-{Based} {Biases}},
	url = {http://arxiv.org/abs/2104.03149},
	abstract = {We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer "What is the color of the sky" with "blue" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts.},
	urldate = {2021-12-25},
	journal = {arXiv:2104.03149 [cs]},
	author = {Dancette, Corentin and Cadene, Remi and Teney, Damien and Cord, Matthieu},
	month = sep,
	year = {2021},
	note = {arXiv: 2104.03149},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, bib},
	annote = {Comment: Accepted at ICCV 2021. Code is available at https://github.com/cdancette/detect-shortcuts},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\LBM6FY78\\2104.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\TH939YK2\\Dancette 等。 - 2021 - Beyond Question-Based Biases Assessing Multimodal.pdf:application/pdf},
}

@article{cho_dealing_2021,
	title = {Dealing with {Missing} {Modalities} in the {Visual} {Question} {Answer}-{Difference} {Prediction} {Task} through {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2104.05965},
	abstract = {In this work, we address the issues of missing modalities that have arisen from the Visual Question Answer-Difference prediction task and find a novel method to solve the task at hand. We address the missing modality-the ground truth answers-that are not present at test time and use a privileged knowledge distillation scheme to deal with the issue of the missing modality. In order to efficiently do so, we first introduce a model, the "Big" Teacher, that takes the image/question/answer triplet as its input and outperforms the baseline, then use a combination of models to distill knowledge to a target network (student) that only takes the image/question pair as its inputs. We experiment our models on the VizWiz and VQA-V2 Answer Difference datasets and show through extensive experimentation and ablation the performances of our method and a diverse possibility for future research.},
	urldate = {2021-12-25},
	journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	author = {Cho, Jae Won and Kim, Dong-Jin and Choi, Jinsoo and Jung, Yunjae and Kweon, In So},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.05965},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, bib},
	annote = {Comment: To appear in CVPR MULA Workshop},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\8EJ7VTXU\\2104.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\CNYEE93F\\Cho 等。 - 2021 - Dealing with Missing Modalities in the Visual Ques.pdf:application/pdf},
}

@article{zou_survey_2020,
	title = {A survey on {VQA}\_Datasets and {Approaches}},
	url = {http://arxiv.org/abs/2105.00421},
	doi = {10.1109/ITCA52113.2020.00069},
	abstract = {Visual question answering (VQA) is a task that combines both the techniques of computer vision and natural language processing. It requires models to answer a text-based question according to the information contained in a visual. In recent years, the research field of VQA has been expanded. Research that focuses on the VQA, examining the reasoning ability and VQA on scientific diagrams, has also been explored more. Meanwhile, more multimodal feature fusion mechanisms have been proposed. This paper will review and analyze existing datasets, metrics, and models proposed for the VQA task.},
	urldate = {2021-12-25},
	journal = {2020 2nd International Conference on Information Technology and Computer Application (ITCA)},
	author = {Zou, Yeyun and Xie, Qiyu},
	month = dec,
	year = {2020},
	note = {arXiv: 2105.00421},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, bib},
	pages = {289--297},
	annote = {Comment: 10 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\PSP7YHKD\\2105.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\3Y2GXA3Q\\Zou 和 Xie - 2020 - A survey on VQA_Datasets and Approaches.pdf:application/pdf},
}

@article{de_jong_grounding_2021,
	title = {Grounding {Complex} {Navigational} {Instructions} {Using} {Scene} {Graphs}},
	url = {http://arxiv.org/abs/2106.01607},
	abstract = {Training a reinforcement learning agent to carry out natural language instructions is limited by the available supervision, i.e. knowing when the instruction has been carried out. We adapt the CLEVR visual question answering dataset to generate complex natural language navigation instructions and accompanying scene graphs, yielding an environment-agnostic supervised dataset. To demonstrate the use of this data set, we map the scenes to the VizDoom environment and use the architecture in {\textbackslash}citet\{gatedattention\} to train an agent to carry out these more complex language instructions.},
	urldate = {2021-12-25},
	journal = {arXiv:2106.01607 [cs]},
	author = {de Jong, Michiel and Krishna, Satyapriya and Agarwal, Anuva},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.01607},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Machine Learning, bib},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1706.07230 by other authors},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\ZF2NH4K5\\2106.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\5WQF6F4E\\de Jong 等。 - 2021 - Grounding Complex Navigational Instructions Using .pdf:application/pdf},
}

@article{damario_how_2021,
	title = {How {Modular} {Should} {Neural} {Module} {Networks} {Be} for {Systematic} {Generalization}?},
	url = {http://arxiv.org/abs/2106.08170},
	abstract = {Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via composition of modules that tackle a sub-task. NMNs are a promising strategy to achieve systematic generalization, i.e. overcoming biasing factors in the training distribution. However, the aspects of NMNs that facilitate systematic generalization are not fully understood. In this paper, we demonstrate that the stage and the degree at which modularity is defined has large influence on systematic generalization. In a series of experiments on three VQA datasets (MNIST with multiple attributes, SQOOP, and CLEVR-CoGenT), our results reveal that tuning the degree of modularity in the network, especially at the image encoder stage, reaches substantially higher systematic generalization. These findings lead to new NMN architectures that outperform previous ones in terms of systematic generalization.},
	urldate = {2021-12-25},
	journal = {arXiv:2106.08170 [cs]},
	author = {D'Amario, Vanessa and Sasaki, Tomotake and Boix, Xavier},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.08170},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, bib},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\Q2PFEUJ9\\2106.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\QSSPSWML\\D'Amario 等。 - 2021 - How Modular Should Neural Module Networks Be for S.pdf:application/pdf},
}

@article{chen_zero-shot_2021,
	title = {Zero-shot {Visual} {Question} {Answering} using {Knowledge} {Graph}},
	url = {http://arxiv.org/abs/2107.05348},
	abstract = {Incorporating external knowledge to Visual Question Answering (VQA) has become a vital practical need. Existing methods mostly adopt pipeline approaches with different components for knowledge matching and extraction, feature learning, etc.However, such pipeline approaches suffer when some component does not perform well, which leads to error propagation and poor overall performance. Furthermore, the majority of existing approaches ignore the answer bias issue -- many answers may have never appeared during training (i.e., unseen answers) in real-word application. To bridge these gaps, in this paper, we propose a Zero-shot VQA algorithm using knowledge graphs and a mask-based learning mechanism for better incorporating external knowledge, and present new answer-based Zero-shot VQA splits for the F-VQA dataset. Experiments show that our method can achieve state-of-the-art performance in Zero-shot VQA with unseen answers, meanwhile dramatically augment existing end-to-end models on the normal F-VQA task.},
	urldate = {2021-12-25},
	journal = {International Semantic Web Conference},
	author = {Chen, Zhuo and Chen, Jiaoyan and Geng, Yuxia and Pan, Jeff Z. and Yuan, Zonggang and Chen, Huajun},
	month = oct,
	year = {2021},
	note = {arXiv: 2107.05348},
	keywords = {Computer Science - Artificial Intelligence, bib},
	annote = {Comment: accepted at the International Semantic Web Conference '21 (ISWC 2021)},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\ZJBDE3E7\\2107.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\HTGF4FD2\\Chen 等。 - 2021 - Zero-shot Visual Question Answering using Knowledg.pdf:application/pdf},
}

@article{banerjee_weakly_2021,
	title = {Weakly {Supervised} {Relative} {Spatial} {Reasoning} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2109.01934},
	abstract = {Vision-and-language (V{\textbackslash}\&L) reasoning necessitates perception of visual concepts such as objects and actions, understanding semantics and language grounding, and reasoning about the interplay between the two modalities. One crucial aspect of visual reasoning is spatial understanding, which involves understanding relative locations of objects, i.e.{\textbackslash} implicitly learning the geometry of the scene. In this work, we evaluate the faithfulness of V{\textbackslash}\&L models to such geometric understanding, by formulating the prediction of pair-wise relative locations of objects as a classification as well as a regression task. Our findings suggest that state-of-the-art transformer-based V{\textbackslash}\&L models lack sufficient abilities to excel at this task. Motivated by this, we design two objectives as proxies for 3D spatial reasoning (SR) -- object centroid estimation, and relative position estimation, and train V{\textbackslash}\&L with weak supervision from off-the-shelf depth estimators. This leads to considerable improvements in accuracy for the "GQA" visual question answering challenge (in fully supervised, few-shot, and O.O.D settings) as well as improvements in relative spatial reasoning. Code and data will be released {\textbackslash}href\{https://github.com/pratyay-banerjee/weak\_sup\_vqa\}\{here\}.},
	urldate = {2021-12-25},
	journal = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	author = {Banerjee, Pratyay and Gokhale, Tejas and Yang, Yezhou and Baral, Chitta},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.01934},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Machine Learning, bib},
	annote = {Comment: Accepted to ICCV 2021. PaperId : ICCV2021-10857 Copyright transferred to IEEE ICCV. DOI will be updated later},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\6W8XZ4SG\\2109.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\NLVRZQZ8\\Banerjee 等。 - 2021 - Weakly Supervised Relative Spatial Reasoning for V.pdf:application/pdf},
}

@article{chappuis_how_2021,
	title = {How to find a good image-text embedding for remote sensing visual question answering?},
	url = {http://arxiv.org/abs/2109.11848},
	abstract = {Visual question answering (VQA) has recently been introduced to remote sensing to make information extraction from overhead imagery more accessible to everyone. VQA considers a question (in natural language, therefore easy to formulate) about an image and aims at providing an answer through a model based on computer vision and natural language processing methods. As such, a VQA model needs to jointly consider visual and textual features, which is frequently done through a fusion step. In this work, we study three different fusion methodologies in the context of VQA for remote sensing and analyse the gains in accuracy with respect to the model complexity. Our findings indicate that more complex fusion mechanisms yield an improved performance, yet that seeking a trade-of between model complexity and performance is worthwhile in practice.},
	urldate = {2021-12-25},
	journal = {arXiv:2109.11848 [cs]},
	author = {Chappuis, Christel and Lobry, Sylvain and Kellenberger, Benjamin and Saux, Bertrand Le and Tuia, Devis},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.11848},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, bib},
	annote = {Comment: 10 pages, 4 figures, presented in the MACLEAN workshop during ECML PKDD 2021},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\82RP4AYV\\2109.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\GT3QXF98\\Chappuis 等。 - 2021 - How to find a good image-text embedding for remote.pdf:application/pdf},
}

@article{chen_counterfactual_2021,
	title = {Counterfactual {Samples} {Synthesizing} and {Training} for {Robust} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2110.01013},
	abstract = {Today's VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to complex model design, these ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visual-explainable and question-sensitive abilities. Specifically, CSST is composed of two parts: Counterfactual Samples Synthesizing (CSS) and Counterfactual Samples Training (CST). CSS generates counterfactual samples by carefully masking critical objects in images or words in questions and assigning pseudo ground-truth answers. CST not only trains the VQA models with both complementary samples to predict respective ground-truth answers, but also urges the VQA models to further distinguish the original samples and superficially similar counterfactual ones. To facilitate the CST training, we propose two variants of supervised contrastive loss for VQA, and design an effective positive and negative sample selection mechanism based on CSS. Extensive experiments have shown the effectiveness of CSST. Particularly, by building on top of model LMH+SAR, we achieve record-breaking performance on all OOD benchmarks.},
	urldate = {2021-12-25},
	journal = {arXiv:2110.01013 [cs]},
	author = {Chen, Long and Zheng, Yuhang and Niu, Yulei and Zhang, Hanwang and Xiao, Jun},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.01013},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Multimedia, bib},
	annote = {Comment: Extension of CVPR'20 work (Counterfactual Samples Synthesizing for Robust Visual Question Answering). arXiv admin note: substantial text overlap with arXiv:2003.06576},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\ZKTYHT4J\\2110.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\FQJL6WUC\\Chen 等。 - 2021 - Counterfactual Samples Synthesizing and Training f.pdf:application/pdf},
}

@article{cao_bilateral_2021,
	title = {Bilateral {Cross}-{Modality} {Graph} {Matching} {Attention} for {Feature} {Fusion} in {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2112.07270},
	abstract = {Answering semantically-complicated questions according to an image is challenging in Visual Question Answering (VQA) task. Although the image can be well represented by deep learning, the question is always simply embedded and cannot well indicate its meaning. Besides, the visual and textual features have a gap for different modalities, it is difficult to align and utilize the cross-modality information. In this paper, we focus on these two problems and propose a Graph Matching Attention (GMA) network. Firstly, it not only builds graph for the image, but also constructs graph for the question in terms of both syntactic and embedding information. Next, we explore the intra-modality relationships by a dual-stage graph encoder and then present a bilateral cross-modality graph matching attention to infer the relationships between the image and the question. The updated cross-modality features are then sent into the answer prediction module for final answer prediction. Experiments demonstrate that our network achieves state-of-the-art performance on the GQA dataset and the VQA 2.0 dataset. The ablation studies verify the effectiveness of each modules in our GMA network.},
	urldate = {2021-12-25},
	journal = {arXiv:2112.07270 [cs]},
	author = {Cao, JianJian and Qin, Xiameng and Zhao, Sanyuan and Shen, Jianbing},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.07270},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, bib},
	annote = {Comment: pre-print, TNNLS, 12 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\JRHWKNSU\\2112.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\6SAVEWQZ\\Cao 等。 - 2021 - Bilateral Cross-Modality Graph Matching Attention .pdf:application/pdf},
}
@@@@