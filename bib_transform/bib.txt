
@article{gan_playing_2021,
	title = {Playing {Lottery} {Tickets} with {Vision} and {Language}},
	url = {http://arxiv.org/abs/2104.11832},
	abstract = {Large-scale pre-training has recently revolutionized vision-and-language (VL) research. Models such as LXMERT and UNITER have significantly lifted the state of the art over a wide range of VL tasks. However, the large number of parameters in such models hinders their application in practice. In parallel, work on the lottery ticket hypothesis (LTH) has shown that deep neural networks contain small matching subnetworks that can achieve on par or even better performance than the dense networks when trained in isolation. In this work, we perform the first empirical study to assess whether such trainable subnetworks also exist in pre-trained VL models. We use UNITER as the main testbed (also test on LXMERT and ViLT), and consolidate 7 representative VL tasks for experiments, including visual question answering, visual commonsense reasoning, visual entailment, referring expression comprehension, image-text retrieval, GQA, and NLVR\${\textasciicircum}2\$. Through comprehensive analysis, we summarize our main findings as follows. (\$i\$) It is difficult to find subnetworks that strictly match the performance of the full model. However, we can find "relaxed" winning tickets at 50\%-70\% sparsity that maintain 99\% of the full accuracy. (\$ii\$) Subnetworks found by task-specific pruning transfer reasonably well to the other tasks, while those found on the pre-training tasks at 60\%/70\% sparsity transfer universally, matching 98\%/96\% of the full accuracy on average over all the tasks. (\$iii\$) Besides UNITER, other models such as LXMERT and ViLT can also play lottery tickets. However, the highest sparsity we can achieve for ViLT is far lower than LXMERT and UNITER (30\% vs. 70\%). (\$iv\$) LTH also remains relevant when using other training methods (e.g., adversarial training).},
	urldate = {2021-12-25},
	journal = {arXiv:2104.11832 [cs]},
	author = {Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Chen, Tianlong and Cheng, Yu and Wang, Shuohang and Liu, Jingjing and Wang, Lijuan and Liu, Zicheng},
	month = dec,
	year = {2021},
	note = {arXiv: 2104.11832},
	keywords = {bib, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted to AAAI 2022},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\IMY5QTIU\\2104.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\KTC9CPY4\\Gan 等。 - 2021 - Playing Lottery Tickets with Vision and Language.pdf:application/pdf},
}

@article{gao_chop_2021,
	title = {Chop {Chop} {BERT}: {Visual} {Question} {Answering} by {Chopping} {VisualBERT}'s {Heads}},
	shorttitle = {Chop {Chop} {BERT}},
	url = {http://arxiv.org/abs/2104.14741},
	abstract = {Vision-and-Language (VL) pre-training has shown great potential on many related downstream tasks, such as Visual Question Answering (VQA), one of the most popular problems in the VL field. All of these pre-trained models (such as VisualBERT, ViLBERT, LXMERT and UNITER) are built with Transformer, which extends the classical attention mechanism to multiple layers and heads. To investigate why and how these models work on VQA so well, in this paper we explore the roles of individual heads and layers in Transformer models when handling \$12\$ different types of questions. Specifically, we manually remove (chop) heads (or layers) from a pre-trained VisualBERT model at a time, and test it on different levels of questions to record its performance. As shown in the interesting echelon shape of the result matrices, experiments reveal different heads and layers are responsible for different question types, with higher-level layers activated by higher-level visual reasoning questions. Based on this observation, we design a dynamic chopping module that can automatically remove heads and layers of the VisualBERT at an instance level when dealing with different questions. Our dynamic chopping module can effectively reduce the parameters of the original model by 50\%, while only damaging the accuracy by less than 1\% on the VQA task.},
	urldate = {2021-12-25},
	journal = {arXiv:2104.14741 [cs]},
	author = {Gao, Chenyu and Zhu, Qi and Wang, Peng and Wu, Qi},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.14741},
	keywords = {68T45, bib, Computer Science - Computer Vision and Pattern Recognition, I.4.8},
	annote = {Comment: 14 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\8X2MTV9A\\2104.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\922SNN2Y\\Gao 等。 - 2021 - Chop Chop BERT Visual Question Answering by Chopp.pdf:application/pdf},
}

@article{gong_cross-modal_2021,
	title = {Cross-{Modal} {Self}-{Attention} with {Multi}-{Task} {Pre}-{Training} for {Medical} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2105.00136},
	abstract = {Due to the severe lack of labeled data, existing methods of medical visual question answering usually rely on transfer learning to obtain effective image feature representation and use cross-modal fusion of visual and linguistic features to achieve question-related answer prediction. These two phases are performed independently and without considering the compatibility and applicability of the pre-trained features for cross-modal fusion. Thus, we reformulate image feature pre-training as a multi-task learning paradigm and witness its extraordinary superiority, forcing it to take into account the applicability of features for the specific image comprehension task. Furthermore, we introduce a cross-modal self-attention{\textasciitilde}(CMSA) module to selectively capture the long-range contextual relevance for more effective fusion of visual and linguistic features. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods. Our code and models are available at https://github.com/haifangong/CMSA-MTPT-4-MedicalVQA.},
	urldate = {2021-12-25},
	journal = {arXiv:2105.00136 [cs]},
	author = {Gong, Haifan and Chen, Guanqi and Liu, Sishuo and Yu, Yizhou and Li, Guanbin},
	month = apr,
	year = {2021},
	note = {arXiv: 2105.00136},
	keywords = {bib, Computer Science - Multimedia},
	annote = {Comment: ICMR '21: ACM International Conference on Multimedia Retrieval, Taipei, Taiwan, August 21-24, 2021},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\78TWEUXP\\2105.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\ZNYMCA28\\Gong 等。 - 2021 - Cross-Modal Self-Attention with Multi-Task Pre-Tra.pdf:application/pdf},
}

@article{do_multiple_2021,
	title = {Multiple {Meta}-model {Quantifying} for {Medical} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2105.08913},
	abstract = {Transfer learning is an important step to extract meaningful features and overcome the data limitation in the medical Visual Question Answering (VQA) task. However, most of the existing medical VQA methods rely on external data for transfer learning, while the meta-data within the dataset is not fully utilized. In this paper, we present a new multiple meta-model quantifying method that effectively learns meta-annotation and leverages meaningful features to the medical VQA task. Our proposed method is designed to increase meta-data by auto-annotation, deal with noisy labels, and output meta-models which provide robust features for medical VQA tasks. Extensively experimental results on two public medical VQA datasets show that our approach achieves superior accuracy in comparison with other state-of-the-art methods, while does not require external data to train meta-models.},
	urldate = {2021-12-25},
	journal = {arXiv:2105.08913 [cs]},
	author = {Do, Tuong and Nguyen, Binh X. and Tjiputra, Erman and Tran, Minh and Tran, Quang D. and Nguyen, Anh},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.08913},
	keywords = {bib, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Provisional accepted in MICCAI 2021},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\STTIPV9D\\2105.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\GQA7XJS8\\Do 等。 - 2021 - Multiple Meta-model Quantifying for Medical Visual.pdf:application/pdf},
}

@article{dey_external_2021,
	title = {External {Knowledge} enabled {Text} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2108.09717},
	abstract = {The open-ended question answering task of Text-VQA requires reading and reasoning about local, often previously unseen, scene-text content of an image to generate answers. In this work, we propose the generalized use of external knowledge to augment our understanding of the said scene-text. We design a framework to extract, validate, and reason with knowledge using a standard multimodal transformer for vision language understanding tasks. Through empirical evidence and qualitative results, we demonstrate how external knowledge can highlight instance-only cues and thus help deal with training data bias, improve answer entity type correctness, and detect multiword named entities. We generate results comparable to the state-of-the-art on two publicly available datasets, under the constraints of similar upstream OCR systems and training data.},
	urldate = {2021-12-25},
	journal = {arXiv:2108.09717 [cs]},
	author = {Dey, Arka Ujjal and Valveny, Ernest and Harit, Gaurav},
	month = oct,
	year = {2021},
	note = {arXiv: 2108.09717},
	keywords = {bib, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	annote = {Comment: Submitted to Neurocomputing},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\G9KK2RS2\\2108.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\XFW25W7W\\Dey 等。 - 2021 - External Knowledge enabled Text Visual Question An.pdf:application/pdf},
}

@article{gamage_improved_2021,
	title = {Improved {RAMEN}: {Towards} {Domain} {Generalization} for {Visual} {Question} {Answering}},
	shorttitle = {Improved {RAMEN}},
	url = {http://arxiv.org/abs/2109.02370},
	urldate = {2021-12-25},
	journal = {arXiv:2109.02370 [cs]},
	author = {Gamage, Bhanuka Manesha Samarasekara Vitharana and Hong, Lim Chern},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.02370},
	keywords = {bib, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 11 pages, 3 figures, 2 tables},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\VSJ7SW7V\\2109.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\G3CTUH3R\\Gamage 和 Hong - 2021 - Improved RAMEN Towards Domain Generalization for .pdf:application/pdf},
}

@article{gat_perceptual_2021,
	title = {Perceptual {Score}: {What} {Data} {Modalities} {Does} {Your} {Model} {Perceive}?},
	shorttitle = {Perceptual {Score}},
	url = {http://arxiv.org/abs/2110.14375},
	abstract = {Machine learning advances in the last decade have relied significantly on large-scale datasets that continue to grow in size. Increasingly, those datasets also contain different data modalities. However, large multi-modal datasets are hard to annotate, and annotations may contain biases that we are often unaware of. Deep-net-based classifiers, in turn, are prone to exploit those biases and to find shortcuts. To study and quantify this concern, we introduce the perceptual score, a metric that assesses the degree to which a model relies on the different subsets of the input features, i.e., modalities. Using the perceptual score, we find a surprisingly consistent trend across four popular datasets: recent, more accurate state-of-the-art multi-modal models for visual question-answering or visual dialog tend to perceive the visual data less than their predecessors. This trend is concerning as answers are hence increasingly inferred from textual cues only. Using the perceptual score also helps to analyze model biases by decomposing the score into data subset contributions. We hope to spur a discussion on the perceptiveness of multi-modal models and also hope to encourage the community working on multi-modal classifiers to start quantifying perceptiveness via the proposed perceptual score.},
	urldate = {2021-12-25},
	journal = {Advances in Neural Information Processing Systems},
	author = {Gat, Itai and Schwartz, Idan and Schwing, Alexander},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.14375},
	keywords = {*, bib, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
	annote = {Comment: Accepted to NeurIPS 2021},
}

@article{garcia-olano_improving_2021,
	title = {Improving and {Diagnosing} {Knowledge}-{Based} {Visual} {Question} {Answering} via {Entity} {Enhanced} {Knowledge} {Injection}},
	url = {http://arxiv.org/abs/2112.06888},
	abstract = {Knowledge-Based Visual Question Answering (KBVQA) is a bi-modal task requiring external world knowledge in order to correctly answer a text question and associated image. Recent single modality text work has shown knowledge injection into pre-trained language models, specifically entity enhanced knowledge graph embeddings, can improve performance on downstream entity-centric tasks. In this work, we empirically study how and whether such methods, applied in a bi-modal setting, can improve an existing VQA system's performance on the KBVQA task. We experiment with two large publicly available VQA datasets, (1) KVQA which contains mostly rare Wikipedia entities and (2) OKVQA which is less entity-centric and more aligned with common sense reasoning. Both lack explicit entity spans and we study the effect of different weakly supervised and manual methods for obtaining them. Additionally we analyze how recently proposed bi-modal and single modal attention explanations are affected by the incorporation of such entity enhanced representations. Our results show substantial improved performance on the KBVQA task without the need for additional costly pre-training and we provide insights for when entity knowledge injection helps improve a model's understanding. We provide code and enhanced datasets for reproducibility.},
	urldate = {2021-12-25},
	journal = {arXiv:2112.06888 [cs]},
	author = {Garcia-Olano, Diego and Onoe, Yasumasa and Ghosh, Joydeep},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.06888},
	keywords = {bib, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\FNMFEILZ\\2112.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\UVFFES9Z\\Garcia-Olano 等。 - 2021 - Improving and Diagnosing Knowledge-Based Visual Qu.pdf:application/pdf},
}

@article{gui_kat_2021,
	title = {{KAT}: {A} {Knowledge} {Augmented} {Transformer} for {Vision}-and-{Language}},
	shorttitle = {{KAT}},
	url = {http://arxiv.org/abs/2112.08614},
	abstract = {The primary focus of recent work with largescale transformers has been on optimizing the amount of information packed into the model's parameters. In this work, we ask a different question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a novel model - Knowledge Augmented Transformer (KAT) - which achieves a strong state-of-the-art result (+6 points absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an end to end encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. An additional benefit of explicit knowledge integration is seen in improved interpretability of model predictions in our analysis.},
	urldate = {2022-01-17},
	journal = {arXiv:2112.08614 [cs]},
	author = {Gui, Liangke and Wang, Borui and Huang, Qiuyuan and Hauptmann, Alex and Bisk, Yonatan and Gao, Jianfeng},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.08614},
	keywords = {bib, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\J9N9W48B\\Gui 等。 - 2021 - KAT A Knowledge Augmented Transformer for Vision-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\V5INAUHY\\2112.html:text/html},
}

@article{eslami_does_2021,
	title = {Does {CLIP} {Benefit} {Visual} {Question} {Answering} in the {Medical} {Domain} as {Much} as it {Does} in the {General} {Domain}?},
	url = {http://arxiv.org/abs/2112.13906},
	abstract = {Contrastive Language--Image Pre-training (CLIP) has shown remarkable success in learning with cross-modal supervision from extensive amounts of image--text pairs collected online. Thus far, the effectiveness of CLIP has been investigated primarily in general-domain multimodal problems. This work evaluates the effectiveness of CLIP for the task of Medical Visual Question Answering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of CLIP for the medical domain based on PubMed articles. Our experiments are conducted on two MedVQA benchmark datasets and investigate two MedVQA methods, MEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via Conditional Reasoning). For each of these, we assess the merits of visual representation learning using PubMedCLIP, the original CLIP, and state-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only on visual data. We open source the code for our MedVQA pipeline and pre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison to MAML's visual encoder. PubMedCLIP achieves the best results with gains in the overall accuracy of up to 3\%. Individual examples illustrate the strengths of PubMedCLIP in comparison to the previously widely used MAML networks. Visual representation learning with language supervision in PubMedCLIP leads to noticeable improvements for MedVQA. Our experiments reveal distributional differences in the two MedVQA benchmark datasets that have not been imparted in previous work and cause different back-end visual encoders in PubMedCLIP to exhibit different behavior on these datasets. Moreover, we witness fundamental performance differences of VQA in general versus medical domains.},
	urldate = {2022-01-17},
	journal = {arXiv:2112.13906 [cs]},
	author = {Eslami, Sedigheh and de Melo, Gerard and Meinel, Christoph},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.13906},
	keywords = {bib, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Taacc\\Zotero\\storage\\LXQEV2S2\\Eslami 等。 - 2021 - Does CLIP Benefit Visual Question Answering in the.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Taacc\\Zotero\\storage\\CJMN39LC\\2112.html:text/html},
}
@@