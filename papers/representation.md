## [Representation](../README.md)
- Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yunhsuan and Li, Zhen and Duerig, Tom,**Scaling {Up} {Visual} and {Vision}-{Language} {Representation} {Learning} {With} {Noisy} {Text} {Supervision}**,arXiv:2102.05918 [cs] 2021 [[Paper]](http://arxiv.org/abs/2102.05918)

- Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng,**{VinVL}: {Making} {Visual} {Representations} {Matter} in {Vision}-{Language} {Models}**,arXiv:2101.00529 [cs] 2021 [[Paper]](http://arxiv.org/abs/2101.00529)

- Li, Wei and Gao, Can and Niu, Guocheng and Xiao, Xinyan and Liu, Hao and Liu, Jiachen and Wu, Hua and Wang, Haifeng,**{UNIMO}: {Towards} {Unified}-{Modal} {Understanding} and {Generation} via {Cross}-{Modal} {Contrastive} {Learning}**,arXiv:2012.15409 [cs] 2020 [[Paper]](http://arxiv.org/abs/2012.15409)
 
- Parcalabescu, Letitia and Gatt, Albert and Frank, Anette and Calixto, Iacer,**Seeing past words: {Testing} the cross-modal capabilities of pretrained {V}\&{L} models**,arXiv:2012.12352 [cs] 2020 [[Paper]](http://arxiv.org/abs/2012.12352)

- Gardères, François and Ziaeefard, Maryam and Abeloos, Baptiste and Lecue, Freddy,**{ConceptBert}: {Concept}-{Aware} {Representation} for {Visual} {Question} {Answering}**,Findings of the Association for Computational Linguistics:EMNLP 2020 2020 [[Paper]](https://www.aclweb.org/anthology/2020.findings-emnlp.44)

- Wang, Jianfeng and Hu, Xiaowei and Zhang, Pengchuan and Li, Xiujun and Wang, Lijuan and Zhang, Lei and Gao, Jianfeng and Liu, Zicheng,**{MiniVLM}: {A} {Smaller} and {Faster} {Vision}-{Language} {Model}**,arXiv:2012.06946 [cs] 2020 [[Paper]](http://arxiv.org/abs/2012.06946)

- Li, Linjie and Gan, Zhe and Liu, Jingjing,**A {Closer} {Look} at the {Robustness} of {Vision}-and-{Language} {Pre}-trained {Models}**,arXiv:2012.08673 [cs] 2020 [[Paper]](http://arxiv.org/abs/2012.08673)

- Bugliarello, Emanuele and Cotterell, Ryan and Okazaki, Naoaki and Elliott, Desmond,**Multimodal {Pretraining} {Unmasked}: {Unifying} the {Vision} and {Language} {BERTs}**,arXiv:2011.15124 [cs] 2020 [[Paper]](http://arxiv.org/abs/2011.15124)

- Khan, Aisha Urooj and Mazaheri, Amir and Lobo, Niels da Vitoria and Shah, Mubarak,**{MMFT}-{BERT}: {Multimodal} {Fusion} {Transformer} with {BERT} {Encodings} for {Visual} {Question} {Answering}**,arXiv:2010.14095 [cs] 2020 [[Paper]](http://arxiv.org/abs/2010.14095)

- Cho, Jaemin and Lu, Jiasen and Schwenk, Dustin and Hajishirzi, Hannaneh and Kembhavi, Aniruddha,**X-{LXMERT}: {Paint}, {Caption} and {Answer} {Questions} with {Multi}-{Modal} {Transformers}**,arXiv:2009.11278 [cs] 2020 [[Paper]](http://arxiv.org/abs/2009.11278)

- Shi, Lei and Shuang, Kai and Geng, Shijie and Su, Peng and Jiang, Zhengkai and Gao, Peng and Fu, Zuohui and de Melo, Gerard and Su, Sen,**Contrastive Visual-Linguistic Pretraining**,arXiv preprint arXiv:2007.13135 2020 [[Paper]](http://arxiv.org/abs/2007.13135)

- Li, Xiujun; Yin, Xi; Li, Chunyuan; Zhang, Pengchuan; Hu, Xiaowei; Zhang, Lei; Wang, Lijuan; Hu, Houdong; Dong, Li; Wei, Furu; Choi, Yejin; Gao, Jianfeng, **Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks**, arXiv:2004.06165   2020   [[Paper]](http://arxiv.org/abs/2004.06165)

- Yu, Fei; Tang, Jiji; Yin, Weichong; Sun, Yu; Tian, Hao; Wu, Hua; Wang, Haifeng, **ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph**, arXiv:2006.16934   2020   [[Paper]](http://arxiv.org/abs/2006.16934)

- Gan, Zhe; Chen, Yen-Chun; Li, Linjie; Zhu, Chen; Cheng, Yu; Liu, Jingjing, **Large-Scale Adversarial Training for Vision-and-Language Representation Learning**, arXiv:2006.06195   2020   [[Paper]](http://arxiv.org/abs/2006.06195)

- Huang, Haoyang; Su, Lin; Qi, Di; Duan, Nan; Cui, Edward; Bharti, Taroon; Zhang, Lei; Wang, Lijuan; Gao, Jianfeng; Liu, Bei; Fu, Jianlong; Zhang, Dongdong; Liu, Xin; Zhou, Ming, **M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training**, arXiv:2006.02635   2020   [[Paper]](http://arxiv.org/abs/2006.02635)

- Chen, Yen-Chun; Li, Linjie; Yu, Licheng; Kholy, Ahmed El; Ahmed, Faisal; Gan, Zhe; Cheng, Yu; Liu, Jingjing, **UNITER: UNiversal Image-TExt Representation Learning**, arXiv:1909.11740   2020   [[Paper]](http://arxiv.org/abs/1909.11740)

- Su, Weijie; Zhu, Xizhou; Cao, Yue; Li, Bin; Lu, Lewei; Wei, Furu; Dai, Jifeng, **VL-BERT: Pre-training of Generic Visual-Linguistic Representations**, arXiv:1908.08530   2020   [[Paper]](http://arxiv.org/abs/1908.08530)

- Lu, Jiasen; Goswami, Vedanuj; Rohrbach, Marcus; Parikh, Devi; Lee, Stefan, **12-in-1: Multi-Task Vision and Language Representation Learning**, CVPR 2020   [[Paper]](http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_12-in-1_Multi-Task_Vision_and_Language_Representation_Learning_CVPR_2020_paper.pdf)

- Lu, Jiasen; Batra, Dhruv; Parikh, Devi; Lee, Stefan, **ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks**, Advances in Neural Information Processing Systems 2019   [[Paper]](http://papers.nips.cc/paper/8297-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks.pdf)

- Zhou, Luowei; Palangi, Hamid; Zhang, Lei; Hu, Houdong; Corso, Jason J.; Gao, Jianfeng, **Unified Vision-Language Pre-Training for Image Captioning and VQA**, AAAI 2019   [[Paper]](http://arxiv.org/abs/1909.11059)

